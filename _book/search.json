[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stats & ML Notes",
    "section": "",
    "text": "Preface\n\n\n\n\n\n\nAbout\n\n\n\nThis is my notes on statistics and machine learning using R."
  },
  {
    "objectID": "stats/normality-test.html#explore-data",
    "href": "stats/normality-test.html#explore-data",
    "title": "Normality Test",
    "section": "Explore Data",
    "text": "Explore Data\n\nglimpse(ToothGrowth)\n#&gt; Rows: 60\n#&gt; Columns: 3\n#&gt; $ len  &lt;dbl&gt; 4.2, 11.5, 7.3, 5.8, 6.4, 10.0, 11.2, 11.2, 5.2, 7.0, 16.5, 16.5,…\n#&gt; $ supp &lt;fct&gt; VC, VC, VC, VC, VC, VC, VC, VC, VC, VC, VC, VC, VC, VC, VC, VC, V…\n#&gt; $ dose &lt;dbl&gt; 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 1.0, 1.0, …\n\n\nskimr::skim(ToothGrowth)\n\n\nData summary\n\n\nName\nToothGrowth\n\n\nNumber of rows\n60\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nsupp\n0\n1\nFALSE\n2\nOJ: 30, VC: 30\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nlen\n0\n1\n18.81\n7.65\n4.2\n13.07\n19.25\n25.27\n33.9\n▅▃▅▇▂\n\n\ndose\n0\n1\n1.17\n0.63\n0.5\n0.50\n1.00\n2.00\n2.0\n▇▇▁▁▇"
  },
  {
    "objectID": "stats/normality-test.html#normality-check",
    "href": "stats/normality-test.html#normality-check",
    "title": "Normality Test",
    "section": "Normality Check",
    "text": "Normality Check\n\n\n\n\n\n\nObjective\n\n\n\nWe want to test if the variable len (tooth length) is normally distributed."
  },
  {
    "objectID": "stats/normality-test.html#visual-method",
    "href": "stats/normality-test.html#visual-method",
    "title": "Normality Test",
    "section": "Visual Method",
    "text": "Visual Method\n\nHistogram\n\nToothGrowth %&gt;% \n  ggplot(aes(len)) +\n  geom_histogram()\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nDensity\n\nggdensity(ToothGrowth$len, fill = \"lightgray\")\n\n\n\n\n\n\nQQ Plot\n\nggqqplot(ToothGrowth$len)\n\n\n\n\n\nToothGrowth %&gt;% \n  ggplot(aes(sample = len)) +\n  geom_qq() +\n  geom_qq_line()"
  },
  {
    "objectID": "stats/normality-test.html#shapiro-wilks-normality-test",
    "href": "stats/normality-test.html#shapiro-wilks-normality-test",
    "title": "Normality Test",
    "section": "Shapiro-Wilk’s normality test",
    "text": "Shapiro-Wilk’s normality test\n\n\n\n\n\n\nHypothesis\n\n\n\n\\(H_0\\) = “sample distribution is normal”\n\n\n\nOne Variable\n\nshapiro.test(ToothGrowth$len)\n#&gt; \n#&gt;  Shapiro-Wilk normality test\n#&gt; \n#&gt; data:  ToothGrowth$len\n#&gt; W = 0.96743, p-value = 0.1091\n\nOr\n\nToothGrowth %&gt;% shapiro_test(len)\n#&gt; # A tibble: 1 × 3\n#&gt;   variable statistic     p\n#&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 len          0.967 0.109\n\nP-value &gt; 0.05; implying that the distribution of the data are not significantly different from normal distribution; therefore, we can assume normality.\n\n\nGrouped Data\n\nToothGrowth %&gt;%\n  group_by(dose) %&gt;%\n  shapiro_test(len)\n#&gt; # A tibble: 3 × 4\n#&gt;    dose variable statistic     p\n#&gt;   &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1   0.5 len          0.941 0.247\n#&gt; 2   1   len          0.931 0.164\n#&gt; 3   2   len          0.978 0.902"
  },
  {
    "objectID": "stats/infer.html#explore-data",
    "href": "stats/infer.html#explore-data",
    "title": "Infer Package Intro",
    "section": "Explore Data",
    "text": "Explore Data\n\nglimpse(gss)\n#&gt; Rows: 500\n#&gt; Columns: 11\n#&gt; $ year    &lt;dbl&gt; 2014, 1994, 1998, 1996, 1994, 1996, 1990, 2016, 2000, 1998, 20…\n#&gt; $ age     &lt;dbl&gt; 36, 34, 24, 42, 31, 32, 48, 36, 30, 33, 21, 30, 38, 49, 25, 56…\n#&gt; $ sex     &lt;fct&gt; male, female, male, male, male, female, female, female, female…\n#&gt; $ college &lt;fct&gt; degree, no degree, degree, no degree, degree, no degree, no de…\n#&gt; $ partyid &lt;fct&gt; ind, rep, ind, ind, rep, rep, dem, ind, rep, dem, dem, ind, de…\n#&gt; $ hompop  &lt;dbl&gt; 3, 4, 1, 4, 2, 4, 2, 1, 5, 2, 4, 3, 4, 4, 2, 2, 3, 2, 1, 2, 5,…\n#&gt; $ hours   &lt;dbl&gt; 50, 31, 40, 40, 40, 53, 32, 20, 40, 40, 23, 52, 38, 72, 48, 40…\n#&gt; $ income  &lt;ord&gt; $25000 or more, $20000 - 24999, $25000 or more, $25000 or more…\n#&gt; $ class   &lt;fct&gt; middle class, working class, working class, working class, mid…\n#&gt; $ finrela &lt;fct&gt; below average, below average, below average, above average, ab…\n#&gt; $ weight  &lt;dbl&gt; 0.8960034, 1.0825000, 0.5501000, 1.0864000, 1.0825000, 1.08640…"
  },
  {
    "objectID": "stats/infer.html#specifying-response-specify",
    "href": "stats/infer.html#specifying-response-specify",
    "title": "Infer Package Intro",
    "section": "Specifying Response specify()",
    "text": "Specifying Response specify()\nSpecify response and explanatory variable as formula or arguments.\n\nContinuous Response\nage (num) ~ partyid (fct)\n\ngss_spec_age_partyid &lt;- gss %&gt;% \n  specify(age ~ partyid)\n#&gt; Dropping unused factor levels DK from the supplied explanatory variable 'partyid'.\n\n# Object Type\nsloop::otype(gss_spec_age_partyid)\n#&gt; [1] \"S3\"\n# Class\nclass(gss_spec_age_partyid)\n#&gt; [1] \"infer\"      \"tbl_df\"     \"tbl\"        \"data.frame\"\n# Print\ngss_spec_age_partyid\n#&gt; Response: age (numeric)\n#&gt; Explanatory: partyid (factor)\n#&gt; # A tibble: 500 × 2\n#&gt;      age partyid\n#&gt;    &lt;dbl&gt; &lt;fct&gt;  \n#&gt;  1    36 ind    \n#&gt;  2    34 rep    \n#&gt;  3    24 ind    \n#&gt;  4    42 ind    \n#&gt;  5    31 rep    \n#&gt;  6    32 rep    \n#&gt;  7    48 dem    \n#&gt;  8    36 ind    \n#&gt;  9    30 rep    \n#&gt; 10    33 dem    \n#&gt; # … with 490 more rows\n\n\n\nCategorical Response\nspecifying for inference on proportions\nyou will need to use the success argument to specify which level of your response variable is a success.\n\ngss %&gt;%\n  specify(response = college, success = \"degree\")\n#&gt; Response: college (factor)\n#&gt; # A tibble: 500 × 1\n#&gt;    college  \n#&gt;    &lt;fct&gt;    \n#&gt;  1 degree   \n#&gt;  2 no degree\n#&gt;  3 degree   \n#&gt;  4 no degree\n#&gt;  5 degree   \n#&gt;  6 no degree\n#&gt;  7 no degree\n#&gt;  8 degree   \n#&gt;  9 degree   \n#&gt; 10 no degree\n#&gt; # … with 490 more rows"
  },
  {
    "objectID": "stats/infer.html#declare-the-null-hypothesis",
    "href": "stats/infer.html#declare-the-null-hypothesis",
    "title": "Infer Package Intro",
    "section": "Declare the NULL Hypothesis",
    "text": "Declare the NULL Hypothesis\ndeclare a null hypothesis using hypothesize().\nnull: “independence” or “point”.\n\nTest Independence\nIf the null hypothesis is that the mean number of hours worked per week in our population is 40, we would write:\n\ngss %&gt;%\n  specify(college ~ partyid, success = \"degree\") %&gt;%\n  hypothesize(null = \"independence\")\n#&gt; Dropping unused factor levels DK from the supplied explanatory variable 'partyid'.\n#&gt; Response: college (factor)\n#&gt; Explanatory: partyid (factor)\n#&gt; Null Hypothesis: independence\n#&gt; # A tibble: 500 × 2\n#&gt;    college   partyid\n#&gt;    &lt;fct&gt;     &lt;fct&gt;  \n#&gt;  1 degree    ind    \n#&gt;  2 no degree rep    \n#&gt;  3 degree    ind    \n#&gt;  4 no degree ind    \n#&gt;  5 degree    rep    \n#&gt;  6 no degree rep    \n#&gt;  7 no degree dem    \n#&gt;  8 degree    ind    \n#&gt;  9 degree    rep    \n#&gt; 10 no degree dem    \n#&gt; # … with 490 more rows\n\n\n\nTest Point Estimate\n\ngss %&gt;%\n  specify(response = hours) %&gt;%\n  hypothesize(null = \"point\", mu = 40)\n#&gt; Response: hours (numeric)\n#&gt; Null Hypothesis: point\n#&gt; # A tibble: 500 × 1\n#&gt;    hours\n#&gt;    &lt;dbl&gt;\n#&gt;  1    50\n#&gt;  2    31\n#&gt;  3    40\n#&gt;  4    40\n#&gt;  5    40\n#&gt;  6    53\n#&gt;  7    32\n#&gt;  8    20\n#&gt;  9    40\n#&gt; 10    40\n#&gt; # … with 490 more rows"
  },
  {
    "objectID": "stats/infer.html#generate-null-distribution",
    "href": "stats/infer.html#generate-null-distribution",
    "title": "Infer Package Intro",
    "section": "generate() NULL distribution",
    "text": "generate() NULL distribution\n\nset.seed(1)\n\ngss %&gt;%\n  specify(response = hours) %&gt;%\n  hypothesize(null = \"point\", mu = 40) %&gt;%\n  generate(reps = 1000, type = \"bootstrap\")\n#&gt; Response: hours (numeric)\n#&gt; Null Hypothesis: point\n#&gt; # A tibble: 500,000 × 2\n#&gt; # Groups:   replicate [1,000]\n#&gt;    replicate hours\n#&gt;        &lt;int&gt; &lt;dbl&gt;\n#&gt;  1         1 46.6 \n#&gt;  2         1 43.6 \n#&gt;  3         1 38.6 \n#&gt;  4         1 28.6 \n#&gt;  5         1 38.6 \n#&gt;  6         1 38.6 \n#&gt;  7         1  6.62\n#&gt;  8         1 78.6 \n#&gt;  9         1 38.6 \n#&gt; 10         1 38.6 \n#&gt; # … with 499,990 more rows"
  },
  {
    "objectID": "stats/infer.html#calculate-summary-stats",
    "href": "stats/infer.html#calculate-summary-stats",
    "title": "Infer Package Intro",
    "section": "Calculate Summary Stats",
    "text": "Calculate Summary Stats\nfind the point estimate\n\nobs_mean &lt;- gss %&gt;%\n  specify(response = hours) %&gt;%\n  calculate(stat = \"mean\")\n\nobs_mean\n#&gt; Response: hours (numeric)\n#&gt; # A tibble: 1 × 1\n#&gt;    stat\n#&gt;   &lt;dbl&gt;\n#&gt; 1  41.4\n\ngenerate a null distribution\n\nnull_dist &lt;- gss %&gt;%\n  specify(response = hours) %&gt;%\n  hypothesize(null = \"point\", mu = 40) %&gt;%\n  generate(reps = 1000, type = \"bootstrap\") %&gt;%\n  calculate(stat = \"mean\")\n\nnull_dist\n#&gt; Response: hours (numeric)\n#&gt; Null Hypothesis: point\n#&gt; # A tibble: 1,000 × 2\n#&gt;    replicate  stat\n#&gt;        &lt;int&gt; &lt;dbl&gt;\n#&gt;  1         1  40.5\n#&gt;  2         2  40.1\n#&gt;  3         3  39.1\n#&gt;  4         4  40.3\n#&gt;  5         5  38.8\n#&gt;  6         6  39.6\n#&gt;  7         7  40.2\n#&gt;  8         8  40.4\n#&gt;  9         9  40.1\n#&gt; 10        10  40.6\n#&gt; # … with 990 more rows"
  },
  {
    "objectID": "stats/infer.html#visualize-null-dist",
    "href": "stats/infer.html#visualize-null-dist",
    "title": "Infer Package Intro",
    "section": "Visualize Null Dist",
    "text": "Visualize Null Dist\n\nnull_dist %&gt;%\n  visualize()\n\n\n\n\nWhere does our sample’s observed statistic lie on this distribution? We can use the obs_stat argument to specify this.\n\nnull_dist %&gt;%\n  visualize() +\n  shade_p_value(obs_stat = obs_mean, direction = \"two-sided\")"
  },
  {
    "objectID": "stats/infer.html#p-value",
    "href": "stats/infer.html#p-value",
    "title": "Infer Package Intro",
    "section": "P-value",
    "text": "P-value\nget a two-tailed p-value\n\np_value &lt;- null_dist %&gt;%\n  get_p_value(obs_stat = obs_mean, direction = \"two-sided\")\n\np_value\n#&gt; # A tibble: 1 × 1\n#&gt;   p_value\n#&gt;     &lt;dbl&gt;\n#&gt; 1   0.038"
  },
  {
    "objectID": "stats/infer.html#confidence-interval",
    "href": "stats/infer.html#confidence-interval",
    "title": "Infer Package Intro",
    "section": "Confidence Interval",
    "text": "Confidence Interval\n\n# generate a distribution like the null distribution, \n# though exclude the null hypothesis from the pipeline\nboot_dist &lt;- gss %&gt;%\n  specify(response = hours) %&gt;%\n  generate(reps = 1000, type = \"bootstrap\") %&gt;%\n  calculate(stat = \"mean\")\n\n# start with the bootstrap distribution\nci &lt;- boot_dist %&gt;%\n  # calculate the confidence interval around the point estimate\n  get_confidence_interval(point_estimate = obs_mean,\n                          # at the 95% confidence level\n                          level = .95,\n                          # using the standard error\n                          type = \"se\")\n\nci\n#&gt; # A tibble: 1 × 2\n#&gt;   lower_ci upper_ci\n#&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1     40.1     42.7\n\n\nboot_dist %&gt;%\n  visualize() +\n  shade_confidence_interval(endpoints = ci)"
  },
  {
    "objectID": "stats/wilcoxon.html#introduction",
    "href": "stats/wilcoxon.html#introduction",
    "title": "Wilcoxon Test",
    "section": "Introduction",
    "text": "Introduction\n\nThe Wilcoxon test is a non-parametric test for comparing 2 groups\nLess powerful than t-test, i.e., more likely to fail to reject the \\(H_0\\) that there is no difference.\n\n\nWhen to use\nData is not normally distributed and the sample size is small (n &lt; 30) (so that central limit theorem not applied)"
  },
  {
    "objectID": "stats/wilcoxon.html#wilcoxon-signed-rank-test-on-paired-samples",
    "href": "stats/wilcoxon.html#wilcoxon-signed-rank-test-on-paired-samples",
    "title": "Wilcoxon Test",
    "section": "Wilcoxon signed rank test on paired samples",
    "text": "Wilcoxon signed rank test on paired samples\n\nData\n\n# Wide format\ndata(\"mice2\", package = \"datarium\")\nhead(mice2, 3)\n#&gt;   id before after\n#&gt; 1  1  187.2 429.5\n#&gt; 2  2  194.2 404.4\n#&gt; 3  3  231.7 405.6\n\nTransform to long\n\nmice2.long &lt;- mice2 %&gt;%\n  gather(key = \"group\", value = \"weight\", before, after)\n\nhead(mice2.long, 3)\n#&gt;   id  group weight\n#&gt; 1  1 before  187.2\n#&gt; 2  2 before  194.2\n#&gt; 3  3 before  231.7\n\n\n\nSummary Stats\n\nmice2.long %&gt;%\n  group_by(group) %&gt;%\n  get_summary_stats(weight, type = \"median_iqr\")\n#&gt; # A tibble: 2 × 5\n#&gt;   group  variable     n median   iqr\n#&gt;   &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 after  weight      10   405   28.3\n#&gt; 2 before weight      10   197.  19.2\n\n\nggpaired(mice2.long, x = \"group\", y = \"weight\", \n         order = c(\"before\", \"after\"),\n         ylab = \"Weight\", xlab = \"Groups\")\n\n\n\n\nThe test assumes that differences between paired samples should be distributed symmetrically around the median.\n\nmice2d &lt;- mice2 %&gt;% \n  mutate(differences = after - before)\n\ngghistogram(mice2d, x = \"differences\", y = \"..density..\", \n            fill = \"steelblue\",bins = 5, add_density = TRUE)\n\n\n\n\n\n\nComputation\n\nwilcox.test(weight ~ group, data = mice2.long, paired = TRUE)\n#&gt; \n#&gt;  Wilcoxon signed rank exact test\n#&gt; \n#&gt; data:  weight by group\n#&gt; V = 55, p-value = 0.001953\n#&gt; alternative hypothesis: true location shift is not equal to 0\n\nOr\n\nstat.test &lt;- mice2.long  %&gt;%\n  wilcox_test(weight ~ group, paired = TRUE) %&gt;%\n  add_significance()\n\nstat.test\n#&gt; # A tibble: 1 × 8\n#&gt;   .y.    group1 group2    n1    n2 statistic       p p.signif\n#&gt; * &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   \n#&gt; 1 weight after  before    10    10        55 0.00195 **\n\n\n\nEffect size\n\nmice2.long  %&gt;%\n  wilcox_effsize(weight ~ group, paired = TRUE)\n#&gt; # A tibble: 1 × 7\n#&gt;   .y.    group1 group2 effsize    n1    n2 magnitude\n#&gt; * &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt;    \n#&gt; 1 weight after  before   0.886    10    10 large\n\n\n\nViz ggplot2\n\nmice2.long %&gt;% \n  ggplot(aes(group, weight, color = group, fill = group)) +\n  geom_boxplot(alpha = 0.4) +\n  geom_jitter() +\n  ggpubr::stat_compare_means(method = \"wilcox.test\",\n                             paired = TRUE, \n                             label.x = 1.5, \n                             label.y = 450, \n                             show.legend = F)\n\n\n\n\n\n\nViz: {ggstatsplot}\n\nlibrary(ggstatsplot)\n\n\nset.seed(123) # Seed for bootstraped CI\n\nggwithinstats( # paired samples\n  data = mice2.long,\n  x = group,\n  y = weight,\n  type = \"nonparametric\", # for wilcoxon\n  centrality.plotting = FALSE # remove median\n)"
  },
  {
    "objectID": "stats/dta.html#survived-vs-fare-any-difference",
    "href": "stats/dta.html#survived-vs-fare-any-difference",
    "title": "Diagnosis Accuracy",
    "section": "Survived vs Fare: Any difference ?",
    "text": "Survived vs Fare: Any difference ?\nLet’s ask the following question: were those people who paid more for their ticket more likely to survive?\n\nggstatsplot::ggbetweenstats(\n  titanic_train,\n  x = Survived,\n  y = Fare\n)\n\n\n\n\nConfirm the difference in Fare between 2 groups.\n\nwilcox.test(titanic_train$Fare ~ titanic_train$Survived)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  titanic_train$Fare by titanic_train$Survived\nW = 57806, p-value &lt; 2.2e-16\nalternative hypothesis: true location shift is not equal to 0"
  },
  {
    "objectID": "stats/dta.html#dta-manual-way",
    "href": "stats/dta.html#dta-manual-way",
    "title": "Diagnosis Accuracy",
    "section": "DTA (Manual Way)",
    "text": "DTA (Manual Way)\nNow let’s ask a slightly different question: can a passenger’s fare price be used to predict where or not they survived?\n\nPrep Data\n\ntitanic_sub &lt;- titanic_train |&gt; \n  select(Fare, Survived) |&gt; \n  mutate(Survived_orig = ifelse(Survived == 1L, \"Lived\", \"Died\")) |&gt; \n  mutate(Survived_pred = ifelse(Fare &gt; 14.45, \"Lived\", \"Died\")) |&gt; \n  mutate(across(starts_with(\"Survived_\"), \n                ~factor(.x, levels = c(\"Lived\", \"Died\"))))\n\nhead(titanic_sub)\n\n     Fare Survived Survived_orig Survived_pred\n1  7.2500        0          Died          Died\n2 71.2833        1         Lived         Lived\n3  7.9250        1         Lived          Died\n4 53.1000        1         Lived         Lived\n5  8.0500        0          Died          Died\n6  8.4583        0          Died          Died\n\n\n\n\nConfusion Matrix\n\ncm &lt;- table(pred = titanic_sub$Survived_pred, \n      orig = titanic_sub$Survived_orig)\n\ncm\n\n       orig\npred    Lived Died\n  Lived   231  220\n  Died    111  329\n\n\n\n\nDiagnostic Accuracy\n\n# True Positive\n(tp &lt;- cm[1, 1])\n\n[1] 231\n\n# False Positive\n(fp &lt;- cm[1, 2])\n\n[1] 220\n\n# False Negative\n(fn &lt;- cm[2, 1])\n\n[1] 111\n\n# True Negative\n(tn &lt;- cm[2, 2])\n\n[1] 329\n\n\n\n# Sense\ntp / (tp + fn)\n\n[1] 0.6754386\n\n# Spec\ntn / (tn + fp)\n\n[1] 0.5992714\n\n# PPV\ntp / (tp + fp)\n\n[1] 0.5121951\n\n# NPV\ntn / (tn + fn)\n\n[1] 0.7477273"
  },
  {
    "objectID": "stats/dta.html#roc-curve",
    "href": "stats/dta.html#roc-curve",
    "title": "Diagnosis Accuracy",
    "section": "ROC Curve",
    "text": "ROC Curve\n\nr1 &lt;- roc(Survived_orig ~ Fare, data = titanic_sub)\n\nSetting levels: control = Lived, case = Died\n\n\nSetting direction: controls &gt; cases\n\nr1\n\n\nCall:\nroc.formula(formula = Survived_orig ~ Fare, data = titanic_sub)\n\nData: Fare in 342 controls (Survived_orig Lived) &gt; 549 cases (Survived_orig Died).\nArea under the curve: 0.6921\n\n\n\n# AUC\n(auc &lt;- auc(r1))\n\nArea under the curve: 0.6921\n\n# Confidence Interval\n(ci &lt;- ci.auc(r1))\n\n95% CI: 0.6567-0.7276 (DeLong)\n\nci_l &lt;- round(ci[1], 2) # Lower\nci_u &lt;- round(ci[3], 2) # Upper\n\n\nhead(r1$thresholds)\n\n[1]      Inf 387.6646 262.6875 254.9479 237.5229 224.6521\n\n\n\nPlot (Base R)\n\nplot(r1, type = \"S\")\n\n\n\n\n\n\nPlot (ggplot2)\n\nlegend_text &lt;- paste0(\n    \"AUC = \", round(auc, 2), \" (95% CI = \", ci_l, \" - \", ci_u, \")\"\n)\n\n\nggroc(r1)+ \n  ggtitle(\"Receiver Operating Characteristic Curve\") +\n  geom_segment(\n    aes(x = 1, xend = 0, y = 0, yend = 1), color = \"grey\", \n    linetype = \"dashed\" ) +\n  scale_y_continuous(expand = c(0, 0)) + \n  scale_x_reverse(expand = c(0, 0)) + \n  annotate(\"text\", x = 0.3, y = 0.05, label = legend_text)\n\nScale for x is already present.\nAdding another scale for x, which will replace the existing scale."
  },
  {
    "objectID": "stats/perf-mat.html#pre-processing",
    "href": "stats/perf-mat.html#pre-processing",
    "title": "Multi-class Performance Matrix",
    "section": "Pre-processing",
    "text": "Pre-processing\n\nset.seed(1)\niris_split &lt;- initial_split(iris, prop = 0.7, strata = Species)\niris_tr &lt;- training(iris_split)\niris_tst &lt;- testing(iris_split)\n\nRecipes\n\niris_rec &lt;- recipe(Species ~ ., data = iris) \n\nModel Spec\n\nmultinom_sim &lt;- multinom_reg(engine = \"nnet\")\n\nWorkflow\n\niris_wf &lt;- workflow(iris_rec, multinom_sim)"
  },
  {
    "objectID": "stats/perf-mat.html#fit-predict",
    "href": "stats/perf-mat.html#fit-predict",
    "title": "Multi-class Performance Matrix",
    "section": "Fit & Predict",
    "text": "Fit & Predict\nFit Model\n\niris_fit &lt;- fit(iris_wf, data = iris_tr)\niris_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: multinom_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nCall:\nnnet::multinom(formula = ..y ~ ., data = data, trace = FALSE)\n\nCoefficients:\n           (Intercept) Sepal.Length Sepal.Width Petal.Length Petal.Width\nversicolor    53.35203     3.845927   -31.93694     10.02870   -3.275975\nvirginica    -57.85131   -23.541980   -41.71563     61.61759   31.272313\n\nResidual Deviance: 0.1554973 \nAIC: 20.1555 \n\n\nPredict\n\niris_res &lt;- broom::augment(iris_fit, new_data = iris_tst)\nhead(iris_res)\n\n# A tibble: 6 × 9\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species .pred_class\n         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;   &lt;fct&gt;      \n1          4.9         3            1.4         0.2 setosa  setosa     \n2          5           3.6          1.4         0.2 setosa  setosa     \n3          5.4         3.7          1.5         0.2 setosa  setosa     \n4          4.8         3            1.4         0.1 setosa  setosa     \n5          5.7         4.4          1.5         0.4 setosa  setosa     \n6          5.4         3.9          1.3         0.4 setosa  setosa     \n# ℹ 3 more variables: .pred_setosa &lt;dbl&gt;, .pred_versicolor &lt;dbl&gt;,\n#   .pred_virginica &lt;dbl&gt;"
  },
  {
    "objectID": "stats/perf-mat.html#multi-class-performace-matrix",
    "href": "stats/perf-mat.html#multi-class-performace-matrix",
    "title": "Multi-class Performance Matrix",
    "section": "Multi-class Performace Matrix",
    "text": "Multi-class Performace Matrix\n\nConfusion Matrix\n\nconf_mat(iris_res, truth = Species, estimate = .pred_class)\n\n            Truth\nPrediction   setosa versicolor virginica\n  setosa         14          0         0\n  versicolor      1         13         0\n  virginica       0          2        15\n\n\nCount number of observed class\n\nclass_totals &lt;- iris_res |&gt; \n  count(Species, name = \"totals\") %&gt;% \n  mutate(class_wts = totals / sum(totals))\n\nclass_totals\n\n# A tibble: 3 × 3\n  Species    totals class_wts\n  &lt;fct&gt;       &lt;int&gt;     &lt;dbl&gt;\n1 setosa         15     0.333\n2 versicolor     15     0.333\n3 virginica      15     0.333\n\n\n\ncell_counts &lt;- \n  iris_res %&gt;% \n  group_by(Species, .pred_class) %&gt;% \n  count() %&gt;% \n  ungroup()\n\ncell_counts\n\n# A tibble: 5 × 3\n  Species    .pred_class     n\n  &lt;fct&gt;      &lt;fct&gt;       &lt;int&gt;\n1 setosa     setosa         14\n2 setosa     versicolor      1\n3 versicolor versicolor     13\n4 versicolor virginica       2\n5 virginica  virginica      15\n\n\n\n# Compute the four sensitivities using 1-vs-all\none_versus_all &lt;- \n  cell_counts %&gt;% \n  filter(Species == .pred_class) %&gt;% \n  full_join(class_totals, by = \"Species\") %&gt;% \n  mutate(sens = n / totals)\n\none_versus_all\n\n# A tibble: 3 × 6\n  Species    .pred_class     n totals class_wts  sens\n  &lt;fct&gt;      &lt;fct&gt;       &lt;int&gt;  &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 setosa     setosa         14     15     0.333 0.933\n2 versicolor versicolor     13     15     0.333 0.867\n3 virginica  virginica      15     15     0.333 1    \n\n\n\n# Three different estimates:\none_versus_all %&gt;% \n  summarize(\n    macro = mean(sens), \n    macro_wts = weighted.mean(sens, class_wts),\n    micro = sum(n) / sum(totals)\n  )\n\n# A tibble: 1 × 3\n  macro macro_wts micro\n  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 0.933     0.933 0.933\n\n\n\nMacro-averaging: computes a set of one-versus-all metrics using the standard two-class statistics. These are averaged.\nMacro-weighted averaging: does the same but the average is weighted by the number of samples in each class.\nMicro-averaging: computes the contribution for each class, aggregates them, then computes a single metric from the aggregates.\n\n\n\n\nMulticlass Doodles"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "ml/tidymod-overview.html#explore-data",
    "href": "ml/tidymod-overview.html#explore-data",
    "title": "Tidymodels Overview",
    "section": "Explore Data",
    "text": "Explore Data\nOutcome: species\n\nglimpse(pen)\n\nRows: 344\nColumns: 5\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n\n\n\npen |&gt;\n  filter(!is.na(sex)) |&gt;\n  ggplot(aes(x     = flipper_length_mm,\n             y     = bill_depth_mm,\n             color = species,\n             size  = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~sex)\n\n\n\n\nFigure 5.1: ?(caption)\n\n\n\n\n\npen |&gt; \n  count(species)\n\n# A tibble: 3 × 2\n  species       n\n  &lt;fct&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\nComplete record rate:\n\nvapply(pen, function(x) mean(!is.na(x)), numeric(1))\n\n          species     bill_depth_mm flipper_length_mm       body_mass_g \n        1.0000000         0.9941860         0.9941860         0.9941860 \n              sex \n        0.9680233"
  },
  {
    "objectID": "ml/tidymod-overview.html#data-budget",
    "href": "ml/tidymod-overview.html#data-budget",
    "title": "Tidymodels Overview",
    "section": "Data Budget",
    "text": "Data Budget\n\nSplit Data\n\nset.seed(123)\npen_split &lt;- initial_split(pen, prop = 0.8, strata = species)\npen_split\n\n&lt;Training/Testing/Total&gt;\n&lt;274/70/344&gt;\n\npen_train &lt;- training(pen_split)\npen_test &lt;- testing(pen_split)\n\n\n\nResample\n10-folded CV, repeated 2 times from the training data\n\nset.seed(123)\npen_folds &lt;- vfold_cv(pen_train, v = 10)\n\nhead(pen_folds)\n\n# A tibble: 6 × 2\n  splits           id    \n  &lt;list&gt;           &lt;chr&gt; \n1 &lt;split [246/28]&gt; Fold01\n2 &lt;split [246/28]&gt; Fold02\n3 &lt;split [246/28]&gt; Fold03\n4 &lt;split [246/28]&gt; Fold04\n5 &lt;split [247/27]&gt; Fold05\n6 &lt;split [247/27]&gt; Fold06"
  },
  {
    "objectID": "ml/tidymod-overview.html#recipes",
    "href": "ml/tidymod-overview.html#recipes",
    "title": "Tidymodels Overview",
    "section": "Recipes",
    "text": "Recipes\n\npen_rec_base &lt;- recipe(species ~ ., data = pen_train) \npen_rec_base\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4"
  },
  {
    "objectID": "ml/tidymod-overview.html#model-spec",
    "href": "ml/tidymod-overview.html#model-spec",
    "title": "Tidymodels Overview",
    "section": "Model Spec",
    "text": "Model Spec\n\nmspec_cls &lt;- list(\n  multi_nnet = multinom_reg(engine = \"nnet\"),\n  multi_glmnet_lasso = multinom_reg(engine = \"glmnet\", \n                                    penalty = 0.1, mixture = 1)\n)\n\nmap(mspec_cls, translate)\n\n$multi_nnet\nMultinomial Regression Model Specification (classification)\n\nComputational engine: nnet \n\nModel fit template:\nnnet::multinom(formula = missing_arg(), data = missing_arg(), \n    trace = FALSE)\n\n$multi_glmnet_lasso\nMultinomial Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = 0.1\n  mixture = 1\n\nComputational engine: glmnet \n\nModel fit template:\nglmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    alpha = 1, family = \"multinomial\")"
  },
  {
    "objectID": "ml/tidymod-overview.html#workflow",
    "href": "ml/tidymod-overview.html#workflow",
    "title": "Tidymodels Overview",
    "section": "Workflow",
    "text": "Workflow\n\nSingle\n\nworkflow(preprocessor = pen_rec_base, spec = mspec_cls$multi_nnet)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: multinom_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nMultinomial Regression Model Specification (classification)\n\nComputational engine: nnet \n\n\n\n\nCombinations\n\npen_wfset &lt;- workflow_set(\n  preproc = list(base = pen_rec_base),\n  models = mspec_cls\n)\n\npen_wfset\n\n# A workflow set/tibble: 2 × 4\n  wflow_id                info             option    result    \n  &lt;chr&gt;                   &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 base_multi_nnet         &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 base_multi_glmnet_lasso &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n\n\n\npen_wf_base &lt;- extract_workflow(pen_wfset, id = \"base_multi_nnet\")\npen_wf_base\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: multinom_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nMultinomial Regression Model Specification (classification)\n\nComputational engine: nnet"
  },
  {
    "objectID": "ml/tidymod-overview.html#other-spec",
    "href": "ml/tidymod-overview.html#other-spec",
    "title": "Tidymodels Overview",
    "section": "Other Spec",
    "text": "Other Spec\n\nPerformance Matric Spec\nA function factory\n\nmet_set_class &lt;- metric_set(\n  accuracy, sensitivity, specificity,\n  mcc # Matthews correlation coefficient\n)\n\nmet_set_mix &lt;- metric_set(roc_auc, accuracy, sensitivity, specificity)\n\n\n\nResamples Control\n\nkeep_pred &lt;- control_resamples(save_pred = TRUE, save_workflow = TRUE)"
  },
  {
    "objectID": "ml/tidymod-overview.html#fit",
    "href": "ml/tidymod-overview.html#fit",
    "title": "Tidymodels Overview",
    "section": "Fit",
    "text": "Fit\n\nUsing: Test Data\n\npen_fit_base &lt;- fit(pen_wf_base, data = pen_test)\npen_fit_base\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: multinom_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nCall:\nnnet::multinom(formula = ..y ~ ., data = data, trace = FALSE)\n\nCoefficients:\n          (Intercept) bill_depth_mm flipper_length_mm  body_mass_g   sexmale\nChinstrap -21.7033319     -0.555911         0.1906063 -0.001596491 0.7168018\nGentoo     -0.4884471     -6.002835         0.2447407  0.011754103 3.8349607\n\nResidual Deviance: 46.77568 \nAIC: 66.77568 \n\n\n\n\nUsing: Resamples\n\npen_fit_fold_base &lt;- fit_resamples(pen_wf_base, \n                                   resamples = pen_folds,\n                                   metrics = met_set_mix,\n                                   control = keep_pred)\nhead(pen_fit_fold_base)\n\n# A tibble: 6 × 5\n  splits           id     .metrics         .notes           .predictions     \n  &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;           \n1 &lt;split [246/28]&gt; Fold01 &lt;tibble [4 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [28 × 7]&gt;\n2 &lt;split [246/28]&gt; Fold02 &lt;tibble [4 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [28 × 7]&gt;\n3 &lt;split [246/28]&gt; Fold03 &lt;tibble [4 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [28 × 7]&gt;\n4 &lt;split [246/28]&gt; Fold04 &lt;tibble [4 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [28 × 7]&gt;\n5 &lt;split [247/27]&gt; Fold05 &lt;tibble [4 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [27 × 7]&gt;\n6 &lt;split [247/27]&gt; Fold06 &lt;tibble [4 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [27 × 7]&gt;"
  },
  {
    "objectID": "ml/tidymod-overview.html#evaluate-predict",
    "href": "ml/tidymod-overview.html#evaluate-predict",
    "title": "Tidymodels Overview",
    "section": "Evaluate & Predict",
    "text": "Evaluate & Predict\n\nUsing: Test set\n\nPredict\n\npen_res_base &lt;- broom::augment(pen_fit_base, new_data = pen_test)\nglimpse(pen_res_base)\n\nRows: 70\nColumns: 9\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, 17.1, 20.7, 18.4, 17.9, 17.8, 21.1…\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, 186, 197, 184, 187, 188, 196, 179, 19…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, 3300, 4500, 3325, 3200, 3300, 4150…\n$ sex               &lt;fct&gt; male, female, female, NA, male, female, female, fema…\n$ .pred_class       &lt;fct&gt; Adelie, Adelie, Chinstrap, NA, Adelie, Adelie, Adeli…\n$ .pred_Adelie      &lt;dbl&gt; 0.9463005, 0.8797500, 0.4329285, NA, 0.8936564, 0.89…\n$ .pred_Chinstrap   &lt;dbl&gt; 0.05369946, 0.12024966, 0.56707149, NA, 0.10634361, …\n$ .pred_Gentoo      &lt;dbl&gt; 1.147426e-09, 3.454101e-07, 6.534172e-11, NA, 2.2390…\n\n\n\n\nMetric\n\nmet_set_class(pen_res_base, \n              truth = species, \n              estimate = .pred_class,\n              estimator = \"macro\" # Macro AVG\n              )\n\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    multiclass     0.809\n2 sensitivity macro          0.727\n3 specificity macro          0.896\n4 mcc         multiclass     0.704\n\n\n\n\nROC\n\nroc_curve(pen_res_base, truth = species, \n          .pred_Adelie, .pred_Chinstrap, .pred_Gentoo) |&gt; \n  autoplot()\n\n\n\n\nGentoo curve is on the top-left see Figure 5.1 for reason.\n\nroc_auc(pen_res_base, truth = species, \n        .pred_Adelie, .pred_Chinstrap, .pred_Gentoo)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc hand_till      0.933\n\n\n\n\n\nUsing Resamples\n\nMetric\n\ncollect_metrics(pen_fit_fold_base)\n\n# A tibble: 4 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    multiclass 0.823    10  0.0227 Preprocessor1_Model1\n2 roc_auc     hand_till  0.918    10  0.0166 Preprocessor1_Model1\n3 sensitivity macro      0.753    10  0.0231 Preprocessor1_Model1\n4 specificity macro      0.904    10  0.0110 Preprocessor1_Model1\n\n\nThese are the resampling estimates averaged over the individual replicates. To get the metrics for each resample, use the option summarize = FALSE.\n\n\nPredictions\nAssessment set predictions:\n\npen_assess_base &lt;- collect_predictions(pen_fit_fold_base, summarize = FALSE)\nhead(pen_assess_base)\n\n# A tibble: 6 × 8\n  id     .pred_Adelie .pred_Chinstrap .pred_Gentoo  .row .pred_class species\n  &lt;chr&gt;         &lt;dbl&gt;           &lt;dbl&gt;        &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;  \n1 Fold01        0.792          0.208      4.02e-25    10 Adelie      Adelie \n2 Fold01        0.691          0.309      3.25e-18    17 Adelie      Adelie \n3 Fold01        0.982          0.0177     7.24e-12    19 Adelie      Adelie \n4 Fold01        0.979          0.0209     2.11e-22    22 Adelie      Adelie \n5 Fold01        0.846          0.154      4.46e-12    46 Adelie      Adelie \n6 Fold01        0.880          0.120      5.82e-14    51 Adelie      Adelie \n# ℹ 1 more variable: .config &lt;chr&gt;\n\n\n.row column is an integer that matches the row of the original training set so that these results can be properly arranged and joined with the original data.\nAveraged Predictions:\n\ncollect_predictions(pen_fit_fold_base, summarize = T) |&gt; head()\n\n# A tibble: 6 × 7\n   .row species .config    .pred_Adelie .pred_Chinstrap .pred_Gentoo .pred_class\n  &lt;int&gt; &lt;fct&gt;   &lt;chr&gt;             &lt;dbl&gt;           &lt;dbl&gt;        &lt;dbl&gt; &lt;fct&gt;      \n1     1 Adelie  Preproces…      NaN            NaN        NaN        Adelie     \n2     2 Adelie  Preproces…        0.514          0.486      1.41e-17 Adelie     \n3     3 Adelie  Preproces…        0.874          0.126      8.77e-21 Adelie     \n4     4 Adelie  Preproces…        0.949          0.0509     1.53e-14 Adelie     \n5     5 Adelie  Preproces…        0.791          0.209      4.07e-13 Adelie     \n6     6 Adelie  Preproces…      NaN            NaN        NaN        Adelie     \n\n\n\n\nROC\n\npen_assess_base |&gt; \n  group_by(id) |&gt; \n  roc_curve(truth = species, .pred_Adelie, .pred_Chinstrap, .pred_Gentoo) |&gt; \n  autoplot()"
  }
]