[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stats & ML Notes",
    "section": "",
    "text": "Preface\n\n\n\n\n\n\nAbout\n\n\n\nThis is my notes on statistics and machine learning using R."
  },
  {
    "objectID": "stats/normality-test.html#explore-data",
    "href": "stats/normality-test.html#explore-data",
    "title": "Normality Test",
    "section": "Explore Data",
    "text": "Explore Data\n\nglimpse(ToothGrowth)\n#&gt; Rows: 60\n#&gt; Columns: 3\n#&gt; $ len  &lt;dbl&gt; 4.2, 11.5, 7.3, 5.8, 6.4, 10.0, 11.2, 11.2, 5.2, 7.0, 16.5, 16.5,…\n#&gt; $ supp &lt;fct&gt; VC, VC, VC, VC, VC, VC, VC, VC, VC, VC, VC, VC, VC, VC, VC, VC, V…\n#&gt; $ dose &lt;dbl&gt; 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 1.0, 1.0, …\n\n\nskimr::skim(ToothGrowth)\n\n\nData summary\n\n\nName\nToothGrowth\n\n\nNumber of rows\n60\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nsupp\n0\n1\nFALSE\n2\nOJ: 30, VC: 30\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nlen\n0\n1\n18.81\n7.65\n4.2\n13.07\n19.25\n25.27\n33.9\n▅▃▅▇▂\n\n\ndose\n0\n1\n1.17\n0.63\n0.5\n0.50\n1.00\n2.00\n2.0\n▇▇▁▁▇"
  },
  {
    "objectID": "stats/normality-test.html#normality-check",
    "href": "stats/normality-test.html#normality-check",
    "title": "Normality Test",
    "section": "Normality Check",
    "text": "Normality Check\n\n\n\n\n\n\nObjective\n\n\n\nWe want to test if the variable len (tooth length) is normally distributed."
  },
  {
    "objectID": "stats/normality-test.html#visual-method",
    "href": "stats/normality-test.html#visual-method",
    "title": "Normality Test",
    "section": "Visual Method",
    "text": "Visual Method\n\nHistogram\n\nToothGrowth %&gt;% \n  ggplot(aes(len)) +\n  geom_histogram()\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nDensity\n\nggdensity(ToothGrowth$len, fill = \"lightgray\")\n\n\n\n\n\n\nQQ Plot\n\nggqqplot(ToothGrowth$len)\n\n\n\n\n\nToothGrowth %&gt;% \n  ggplot(aes(sample = len)) +\n  geom_qq() +\n  geom_qq_line()"
  },
  {
    "objectID": "stats/normality-test.html#shapiro-wilks-normality-test",
    "href": "stats/normality-test.html#shapiro-wilks-normality-test",
    "title": "Normality Test",
    "section": "Shapiro-Wilk’s normality test",
    "text": "Shapiro-Wilk’s normality test\n\n\n\n\n\n\nHypothesis\n\n\n\n\\(H_0\\) = “sample distribution is normal”\n\n\n\nOne Variable\n\nshapiro.test(ToothGrowth$len)\n#&gt; \n#&gt;  Shapiro-Wilk normality test\n#&gt; \n#&gt; data:  ToothGrowth$len\n#&gt; W = 0.96743, p-value = 0.1091\n\nOr\n\nToothGrowth %&gt;% shapiro_test(len)\n#&gt; # A tibble: 1 × 3\n#&gt;   variable statistic     p\n#&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 len          0.967 0.109\n\nP-value &gt; 0.05; implying that the distribution of the data are not significantly different from normal distribution; therefore, we can assume normality.\n\n\nGrouped Data\n\nToothGrowth %&gt;%\n  group_by(dose) %&gt;%\n  shapiro_test(len)\n#&gt; # A tibble: 3 × 4\n#&gt;    dose variable statistic     p\n#&gt;   &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1   0.5 len          0.941 0.247\n#&gt; 2   1   len          0.931 0.164\n#&gt; 3   2   len          0.978 0.902"
  },
  {
    "objectID": "stats/infer.html#explore-data",
    "href": "stats/infer.html#explore-data",
    "title": "Infer Package Intro",
    "section": "Explore Data",
    "text": "Explore Data\n\nglimpse(gss)\n#&gt; Rows: 500\n#&gt; Columns: 11\n#&gt; $ year    &lt;dbl&gt; 2014, 1994, 1998, 1996, 1994, 1996, 1990, 2016, 2000, 1998, 20…\n#&gt; $ age     &lt;dbl&gt; 36, 34, 24, 42, 31, 32, 48, 36, 30, 33, 21, 30, 38, 49, 25, 56…\n#&gt; $ sex     &lt;fct&gt; male, female, male, male, male, female, female, female, female…\n#&gt; $ college &lt;fct&gt; degree, no degree, degree, no degree, degree, no degree, no de…\n#&gt; $ partyid &lt;fct&gt; ind, rep, ind, ind, rep, rep, dem, ind, rep, dem, dem, ind, de…\n#&gt; $ hompop  &lt;dbl&gt; 3, 4, 1, 4, 2, 4, 2, 1, 5, 2, 4, 3, 4, 4, 2, 2, 3, 2, 1, 2, 5,…\n#&gt; $ hours   &lt;dbl&gt; 50, 31, 40, 40, 40, 53, 32, 20, 40, 40, 23, 52, 38, 72, 48, 40…\n#&gt; $ income  &lt;ord&gt; $25000 or more, $20000 - 24999, $25000 or more, $25000 or more…\n#&gt; $ class   &lt;fct&gt; middle class, working class, working class, working class, mid…\n#&gt; $ finrela &lt;fct&gt; below average, below average, below average, above average, ab…\n#&gt; $ weight  &lt;dbl&gt; 0.8960034, 1.0825000, 0.5501000, 1.0864000, 1.0825000, 1.08640…"
  },
  {
    "objectID": "stats/infer.html#specifying-response-specify",
    "href": "stats/infer.html#specifying-response-specify",
    "title": "Infer Package Intro",
    "section": "Specifying Response specify()",
    "text": "Specifying Response specify()\nSpecify response and explanatory variable as formula or arguments.\n\nContinuous Response\nage (num) ~ partyid (fct)\n\ngss_spec_age_partyid &lt;- gss %&gt;% \n  specify(age ~ partyid)\n#&gt; Dropping unused factor levels DK from the supplied explanatory variable 'partyid'.\n\n# Object Type\nsloop::otype(gss_spec_age_partyid)\n#&gt; [1] \"S3\"\n# Class\nclass(gss_spec_age_partyid)\n#&gt; [1] \"infer\"      \"tbl_df\"     \"tbl\"        \"data.frame\"\n# Print\ngss_spec_age_partyid\n#&gt; Response: age (numeric)\n#&gt; Explanatory: partyid (factor)\n#&gt; # A tibble: 500 × 2\n#&gt;      age partyid\n#&gt;    &lt;dbl&gt; &lt;fct&gt;  \n#&gt;  1    36 ind    \n#&gt;  2    34 rep    \n#&gt;  3    24 ind    \n#&gt;  4    42 ind    \n#&gt;  5    31 rep    \n#&gt;  6    32 rep    \n#&gt;  7    48 dem    \n#&gt;  8    36 ind    \n#&gt;  9    30 rep    \n#&gt; 10    33 dem    \n#&gt; # … with 490 more rows\n\n\n\nCategorical Response\nspecifying for inference on proportions\nyou will need to use the success argument to specify which level of your response variable is a success.\n\ngss %&gt;%\n  specify(response = college, success = \"degree\")\n#&gt; Response: college (factor)\n#&gt; # A tibble: 500 × 1\n#&gt;    college  \n#&gt;    &lt;fct&gt;    \n#&gt;  1 degree   \n#&gt;  2 no degree\n#&gt;  3 degree   \n#&gt;  4 no degree\n#&gt;  5 degree   \n#&gt;  6 no degree\n#&gt;  7 no degree\n#&gt;  8 degree   \n#&gt;  9 degree   \n#&gt; 10 no degree\n#&gt; # … with 490 more rows"
  },
  {
    "objectID": "stats/infer.html#declare-the-null-hypothesis",
    "href": "stats/infer.html#declare-the-null-hypothesis",
    "title": "Infer Package Intro",
    "section": "Declare the NULL Hypothesis",
    "text": "Declare the NULL Hypothesis\ndeclare a null hypothesis using hypothesize().\nnull: “independence” or “point”.\n\nTest Independence\nIf the null hypothesis is that the mean number of hours worked per week in our population is 40, we would write:\n\ngss %&gt;%\n  specify(college ~ partyid, success = \"degree\") %&gt;%\n  hypothesize(null = \"independence\")\n#&gt; Dropping unused factor levels DK from the supplied explanatory variable 'partyid'.\n#&gt; Response: college (factor)\n#&gt; Explanatory: partyid (factor)\n#&gt; Null Hypothesis: independence\n#&gt; # A tibble: 500 × 2\n#&gt;    college   partyid\n#&gt;    &lt;fct&gt;     &lt;fct&gt;  \n#&gt;  1 degree    ind    \n#&gt;  2 no degree rep    \n#&gt;  3 degree    ind    \n#&gt;  4 no degree ind    \n#&gt;  5 degree    rep    \n#&gt;  6 no degree rep    \n#&gt;  7 no degree dem    \n#&gt;  8 degree    ind    \n#&gt;  9 degree    rep    \n#&gt; 10 no degree dem    \n#&gt; # … with 490 more rows\n\n\n\nTest Point Estimate\n\ngss %&gt;%\n  specify(response = hours) %&gt;%\n  hypothesize(null = \"point\", mu = 40)\n#&gt; Response: hours (numeric)\n#&gt; Null Hypothesis: point\n#&gt; # A tibble: 500 × 1\n#&gt;    hours\n#&gt;    &lt;dbl&gt;\n#&gt;  1    50\n#&gt;  2    31\n#&gt;  3    40\n#&gt;  4    40\n#&gt;  5    40\n#&gt;  6    53\n#&gt;  7    32\n#&gt;  8    20\n#&gt;  9    40\n#&gt; 10    40\n#&gt; # … with 490 more rows"
  },
  {
    "objectID": "stats/infer.html#generate-null-distribution",
    "href": "stats/infer.html#generate-null-distribution",
    "title": "Infer Package Intro",
    "section": "generate() NULL distribution",
    "text": "generate() NULL distribution\n\nset.seed(1)\n\ngss %&gt;%\n  specify(response = hours) %&gt;%\n  hypothesize(null = \"point\", mu = 40) %&gt;%\n  generate(reps = 1000, type = \"bootstrap\")\n#&gt; Response: hours (numeric)\n#&gt; Null Hypothesis: point\n#&gt; # A tibble: 500,000 × 2\n#&gt; # Groups:   replicate [1,000]\n#&gt;    replicate hours\n#&gt;        &lt;int&gt; &lt;dbl&gt;\n#&gt;  1         1 46.6 \n#&gt;  2         1 43.6 \n#&gt;  3         1 38.6 \n#&gt;  4         1 28.6 \n#&gt;  5         1 38.6 \n#&gt;  6         1 38.6 \n#&gt;  7         1  6.62\n#&gt;  8         1 78.6 \n#&gt;  9         1 38.6 \n#&gt; 10         1 38.6 \n#&gt; # … with 499,990 more rows"
  },
  {
    "objectID": "stats/infer.html#calculate-summary-stats",
    "href": "stats/infer.html#calculate-summary-stats",
    "title": "Infer Package Intro",
    "section": "Calculate Summary Stats",
    "text": "Calculate Summary Stats\nfind the point estimate\n\nobs_mean &lt;- gss %&gt;%\n  specify(response = hours) %&gt;%\n  calculate(stat = \"mean\")\n\nobs_mean\n#&gt; Response: hours (numeric)\n#&gt; # A tibble: 1 × 1\n#&gt;    stat\n#&gt;   &lt;dbl&gt;\n#&gt; 1  41.4\n\ngenerate a null distribution\n\nnull_dist &lt;- gss %&gt;%\n  specify(response = hours) %&gt;%\n  hypothesize(null = \"point\", mu = 40) %&gt;%\n  generate(reps = 1000, type = \"bootstrap\") %&gt;%\n  calculate(stat = \"mean\")\n\nnull_dist\n#&gt; Response: hours (numeric)\n#&gt; Null Hypothesis: point\n#&gt; # A tibble: 1,000 × 2\n#&gt;    replicate  stat\n#&gt;        &lt;int&gt; &lt;dbl&gt;\n#&gt;  1         1  40.5\n#&gt;  2         2  40.1\n#&gt;  3         3  39.1\n#&gt;  4         4  40.3\n#&gt;  5         5  38.8\n#&gt;  6         6  39.6\n#&gt;  7         7  40.2\n#&gt;  8         8  40.4\n#&gt;  9         9  40.1\n#&gt; 10        10  40.6\n#&gt; # … with 990 more rows"
  },
  {
    "objectID": "stats/infer.html#visualize-null-dist",
    "href": "stats/infer.html#visualize-null-dist",
    "title": "Infer Package Intro",
    "section": "Visualize Null Dist",
    "text": "Visualize Null Dist\n\nnull_dist %&gt;%\n  visualize()\n\n\n\n\nWhere does our sample’s observed statistic lie on this distribution? We can use the obs_stat argument to specify this.\n\nnull_dist %&gt;%\n  visualize() +\n  shade_p_value(obs_stat = obs_mean, direction = \"two-sided\")"
  },
  {
    "objectID": "stats/infer.html#p-value",
    "href": "stats/infer.html#p-value",
    "title": "Infer Package Intro",
    "section": "P-value",
    "text": "P-value\nget a two-tailed p-value\n\np_value &lt;- null_dist %&gt;%\n  get_p_value(obs_stat = obs_mean, direction = \"two-sided\")\n\np_value\n#&gt; # A tibble: 1 × 1\n#&gt;   p_value\n#&gt;     &lt;dbl&gt;\n#&gt; 1   0.038"
  },
  {
    "objectID": "stats/infer.html#confidence-interval",
    "href": "stats/infer.html#confidence-interval",
    "title": "Infer Package Intro",
    "section": "Confidence Interval",
    "text": "Confidence Interval\n\n# generate a distribution like the null distribution, \n# though exclude the null hypothesis from the pipeline\nboot_dist &lt;- gss %&gt;%\n  specify(response = hours) %&gt;%\n  generate(reps = 1000, type = \"bootstrap\") %&gt;%\n  calculate(stat = \"mean\")\n\n# start with the bootstrap distribution\nci &lt;- boot_dist %&gt;%\n  # calculate the confidence interval around the point estimate\n  get_confidence_interval(point_estimate = obs_mean,\n                          # at the 95% confidence level\n                          level = .95,\n                          # using the standard error\n                          type = \"se\")\n\nci\n#&gt; # A tibble: 1 × 2\n#&gt;   lower_ci upper_ci\n#&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1     40.1     42.7\n\n\nboot_dist %&gt;%\n  visualize() +\n  shade_confidence_interval(endpoints = ci)"
  },
  {
    "objectID": "stats/wilcoxon.html#introduction",
    "href": "stats/wilcoxon.html#introduction",
    "title": "Wilcoxon Test",
    "section": "Introduction",
    "text": "Introduction\n\nThe Wilcoxon test is a non-parametric test for comparing 2 groups\nLess powerful than t-test, i.e., more likely to fail to reject the \\(H_0\\) that there is no difference.\n\n\nWhen to use\nData is not normally distributed and the sample size is small (n &lt; 30) (so that central limit theorem not applied)"
  },
  {
    "objectID": "stats/wilcoxon.html#wilcoxon-signed-rank-test-on-paired-samples",
    "href": "stats/wilcoxon.html#wilcoxon-signed-rank-test-on-paired-samples",
    "title": "Wilcoxon Test",
    "section": "Wilcoxon signed rank test on paired samples",
    "text": "Wilcoxon signed rank test on paired samples\n\nData\n\n# Wide format\ndata(\"mice2\", package = \"datarium\")\nhead(mice2, 3)\n#&gt;   id before after\n#&gt; 1  1  187.2 429.5\n#&gt; 2  2  194.2 404.4\n#&gt; 3  3  231.7 405.6\n\nTransform to long\n\nmice2.long &lt;- mice2 %&gt;%\n  gather(key = \"group\", value = \"weight\", before, after)\n\nhead(mice2.long, 3)\n#&gt;   id  group weight\n#&gt; 1  1 before  187.2\n#&gt; 2  2 before  194.2\n#&gt; 3  3 before  231.7\n\n\n\nSummary Stats\n\nmice2.long %&gt;%\n  group_by(group) %&gt;%\n  get_summary_stats(weight, type = \"median_iqr\")\n#&gt; # A tibble: 2 × 5\n#&gt;   group  variable     n median   iqr\n#&gt;   &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 after  weight      10   405   28.3\n#&gt; 2 before weight      10   197.  19.2\n\n\nggpaired(mice2.long, x = \"group\", y = \"weight\", \n         order = c(\"before\", \"after\"),\n         ylab = \"Weight\", xlab = \"Groups\")\n\n\n\n\nThe test assumes that differences between paired samples should be distributed symmetrically around the median.\n\nmice2d &lt;- mice2 %&gt;% \n  mutate(differences = after - before)\n\ngghistogram(mice2d, x = \"differences\", y = \"..density..\", \n            fill = \"steelblue\",bins = 5, add_density = TRUE)\n\n\n\n\n\n\nComputation\n\nwilcox.test(weight ~ group, data = mice2.long, paired = TRUE)\n#&gt; \n#&gt;  Wilcoxon signed rank exact test\n#&gt; \n#&gt; data:  weight by group\n#&gt; V = 55, p-value = 0.001953\n#&gt; alternative hypothesis: true location shift is not equal to 0\n\nOr\n\nstat.test &lt;- mice2.long  %&gt;%\n  wilcox_test(weight ~ group, paired = TRUE) %&gt;%\n  add_significance()\n\nstat.test\n#&gt; # A tibble: 1 × 8\n#&gt;   .y.    group1 group2    n1    n2 statistic       p p.signif\n#&gt; * &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   \n#&gt; 1 weight after  before    10    10        55 0.00195 **\n\n\n\nEffect size\n\nmice2.long  %&gt;%\n  wilcox_effsize(weight ~ group, paired = TRUE)\n#&gt; # A tibble: 1 × 7\n#&gt;   .y.    group1 group2 effsize    n1    n2 magnitude\n#&gt; * &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt;    \n#&gt; 1 weight after  before   0.886    10    10 large\n\n\n\nViz ggplot2\n\nmice2.long %&gt;% \n  ggplot(aes(group, weight, color = group, fill = group)) +\n  geom_boxplot(alpha = 0.4) +\n  geom_jitter() +\n  ggpubr::stat_compare_means(method = \"wilcox.test\",\n                             paired = TRUE, \n                             label.x = 1.5, \n                             label.y = 450, \n                             show.legend = F)\n\n\n\n\n\n\nViz: {ggstatsplot}\n\nlibrary(ggstatsplot)\n\n\nset.seed(123) # Seed for bootstraped CI\n\nggwithinstats( # paired samples\n  data = mice2.long,\n  x = group,\n  y = weight,\n  type = \"nonparametric\", # for wilcoxon\n  centrality.plotting = FALSE # remove median\n)"
  },
  {
    "objectID": "stats/sam-size.html#overview",
    "href": "stats/sam-size.html#overview",
    "title": "Sample Size (Traditional)",
    "section": "Overview",
    "text": "Overview\n\n\n\nPackages for Sample Size\n\n\n\nConventional Effect Size\n\ncohen.ES(test = \"t\", size = \"medium\")\n\n\n     Conventional effect size from Cohen (1982) \n\n           test = t\n           size = medium\n    effect.size = 0.5"
  },
  {
    "objectID": "stats/sam-size.html#two-means-t-test",
    "href": "stats/sam-size.html#two-means-t-test",
    "title": "Sample Size (Traditional)",
    "section": "Two Means T-test",
    "text": "Two Means T-test\ntests if a mean from one group is different from the mean of another group for a normally distributed variable. AKA, testing to see if the difference in means is different from zero.\nEffect size for t-test: 0.2 = small, 0.5 = medium, 0.8 = large effect sizes\n\\[\nd = \\frac{\\mu_1 - \\mu_2}{s_{pooled}}\n\\] The pooled standard deviation ( \\(s_{pooled}\\) ) is calculated as:\n\\[\ns\\_{pooled} = \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}}\n\\] ### Ex 1: Caloric Intake\n\nFind N\nYou are interested in determining if the average daily caloric intake different between men and women. You collected trial data and found the:\n\naverage caloric intake for males to be 2350.2 (SD=258)\nfemales had intake of 1872.4 (SD=420).\n\n(Don’t know N for each group)\n\nsd_pooled_1 &lt;- sqrt((258^2 + 420^2)/2)\nsd_pooled_1\n\n[1] 348.5427\n\neff_size1 &lt;- (2350.2 - 1872.4) / sd_pooled_1\neff_size1\n\n[1] 1.370851\n\n\n\npwr.t.test(d = eff_size1, \n           sig.level=0.05, \n           power=0.80, type= \"two.sample\", alternative=\"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 9.417703\n              d = 1.370851\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\nSimulate\n\ndef_1 &lt;- simstudy::defData(\n  varname = \"intake_male\", formula = 2350.2,\n  variance = 258^2\n) |&gt;\n  simstudy::defData(\n    varname = \"intake_female\", formula = 1872.4,\n    variance = 420^2\n  )\n\nset.seed(123)\ndf_intake &lt;- genData(n = 10, def_1) |&gt; \n  pivot_longer(cols = starts_with(\"intake\"), names_to = \"gender\", \n               values_to = \"intake\", names_prefix = \"intake_\") |&gt; \n  dplyr::select(-id)\n\ndf_intake |&gt; \n  group_by(gender) |&gt; \n  rstatix::get_summary_stats(type = \"mean_sd\")\n\n# A tibble: 2 × 5\n  gender variable     n  mean    sd\n  &lt;chr&gt;  &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 female intake      10 1960.  436.\n2 male   intake      10 2369.  246.\n\n\n\nrstatix::t_test(df_intake, intake ~ gender) # Sig\n\n# A tibble: 1 × 8\n  .y.    group1 group2    n1    n2 statistic    df      p\n* &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 intake female male      10    10     -2.59  14.2 0.0214\n\n\n\n\nEx 2\nYou are interested in determining if the average protein level in blood different between men and women. You collected the following trial data on protein level (grams/deciliter).\n\nprot_samp &lt;- data.frame(\n  gender = c(rep(\"M\", 8), rep(\"F\", 8)),\n  prot = c(\n    c(1.8, 5.8, 7.1, 4.6, 5.5, 2.4, 8.3, 1.2), # Male\n    c(9.5, 2.6, 3.7, 4.7, 6.4, 8.4, 3.1, 1.4) # Female\n  )\n)\n\n\nprot_samp |&gt; \n  group_by(gender) |&gt; \n  summarise(mean = mean(prot), sd = sd(prot))\n\n# A tibble: 2 × 3\n  gender  mean    sd\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 F       4.98  2.88\n2 M       4.59  2.58\n\n\n\neff_size2 &lt;- (4.9750 - 4.5875   ) / sqrt((2.875388^2 + 2.575399^2) / 2)\neff_size2\n\n[1] 0.1419665\n\n\n\npwr.t.test(d = eff_size2, sig.level=0.05, \n           power=0.80, type= \"two.sample\", alternative=\"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 779.8317\n              d = 0.1419665\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group"
  },
  {
    "objectID": "stats/sam-size.html#anova",
    "href": "stats/sam-size.html#anova",
    "title": "Sample Size (Traditional)",
    "section": "ANOVA",
    "text": "ANOVA\n\nEffect Size of ANOVA (f)\nPartial Eta Squared (η2) = SStreat / SStotal\nf = √((η2 /(1- η2)\nTotal Sum of Squares (SStotal):\n\\[\n\\text{SStotal} = \\sum (Y_i - \\bar{Y})^2\n\\]\nwhere ( \\(Y_i\\) ) are the individual data points and ( \\(\\bar{Y}\\) ) is the overall mean.\nTreatment Sum of Squares (SStreat):\n\\[\n\\text{SStreat} = \\sum n_j (\\bar{Y}_j - \\bar{Y})^2\n\\]\nwhere ( \\(n_j\\) ) is the number of observations in each group, ( \\(\\bar{Y}_j\\) ) is the mean of each group, and ( \\(\\bar{Y}\\) ) is the overall mean.\n\n\nEx 1: Sx Option\nYou are interested in determining there is a difference in weight lost between 4 different surgery options. You collect the following trial data of weight lost in pounds\n\nsx_opt &lt;- data.frame(\n  op1 = c(6.3, 2.8, 7.8, 7.9, 4.9),\n  op2 = c(9.9, 4.1, 3.9, 6.3, 6.9),\n  op3 = c(5.1, 2.9, 3.6, 5.7, 4.5),\n  op4 = c(1, 2.8, 4.8, 3.9, 1.6)\n) |&gt; \n  pivot_longer(cols = everything(), names_to = \"op\", values_to = \"wt_loss\")\n\n\nsx_opt_aov &lt;- aov(wt_loss ~ op, data = sx_opt)\nsx_opt_aov_tbl &lt;- summary(sx_opt_aov)\nsx_opt_aov_tbl\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \nop           3  37.13  12.375    3.46 0.0414 *\nResiduals   16  57.22   3.576                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nSStreat\n\\[\n\\text{SStreat} = \\sum n_j (\\bar{Y}_j - \\bar{Y})^2\n\\]\n\nSStreat &lt;- sx_opt_aov_tbl[[1]][\"op\", \"Sum Sq\"]\nSStreat\n\n[1] 37.1255\n\n\nOr\n\nsx_opt |&gt; \n  group_by(op) |&gt; \n  summarise(mean_gr = mean(wt_loss), n = n()) |&gt; \n  mutate(SStx_gr = n * (mean_gr - mean(sx_opt$wt_loss))^2) |&gt; \n  summarise(SStx = sum(SStx_gr))\n\n# A tibble: 1 × 1\n   SStx\n  &lt;dbl&gt;\n1  37.1\n\n\n\n\nSStotal\n\nSStotal &lt;- sum((sx_opt$wt_loss - mean(sx_opt$wt_loss))^2)\nSStotal\n\n[1] 94.3455\n\n\nOr\n\nSStotal &lt;- sum(sx_opt_aov_tbl[[1]][, \"Sum Sq\"])\nSStotal\n\n[1] 94.3455\n\n\n\n\nEffect Size (f)\nPartial Eta Squared (η2) = SStreat / SStotal\nf = √((η2 /(1- η2)\n\nn_squared &lt;- SStreat / SStotal\neff_size3 &lt;- sqrt(n_squared / (1 - n_squared))\neff_size3\n\n[1] 0.8054939\n\n\n\n\nFind N\n\npwr.anova.test(k = 4, f = eff_size3, sig.level=0.05, power =0.80 )\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 5.287432\n              f = 0.8054939\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group"
  },
  {
    "objectID": "stats/sam-size.html#two-prop",
    "href": "stats/sam-size.html#two-prop",
    "title": "Sample Size (Traditional)",
    "section": "Two Prop",
    "text": "Two Prop\n\nEx 1: Stat Scores\nYou are interested in determining if the expected proportion (P1) of students passing a stats course taught by psychology teachers is different than the observed proportion (P2) of students passing the same stats class taught by biology teachers. You collected the following data of passed tests.\n\nstat_course &lt;- data.frame(\n  Psychology = c(\"Yes\", \"Yes\", \"Yes\", \"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\"),\n  Biology = c(\"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"Yes\")\n) \n\ntable(stat_course$Psychology, stat_course$Biology)\n\n     \n      No Yes\n  No   0   3\n  Yes  4   3\n\n\n\np1 &lt;- (4 + 3) / 10\np2 &lt;-  (3 + 3) / 10\n\neff_size4 = 2*asin(sqrt(p2))-2*asin(sqrt(p1))\neff_size4\n\n[1] -0.2101589\n\n\n\npwr.2p.test(h= eff_size4, sig.level=0.05, power=0.80,\n            alternative=\"two.sided\")\n\n\n     Difference of proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.2101589\n              n = 355.4193\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: same sample sizes"
  },
  {
    "objectID": "stats/sam-size.html#chi-squared",
    "href": "stats/sam-size.html#chi-squared",
    "title": "Sample Size (Traditional)",
    "section": "Chi-Squared",
    "text": "Chi-Squared\nDescription: Extension of proportions test, which asks if table of observed values are any different from a table of expected ones. Also called Goodness-of-fit test.\n\nEx 1\nYou are interested in determining if the ethnic ratios in a company differ by gender. You collect the following trial data from 200 employees.\n\nemployee &lt;- data.frame(\n  male = c(rep(\"White\", 0.6 * 100), rep(\"Black\", 0.25 * 100), \n           rep(\"Am\", 0.01 * 100), rep(\"Asian\", 0.14 * 100)),\n  female = c(rep(\"White\", 0.65 * 100), rep(\"Black\", 0.21 * 100), \n           rep(\"Am\", 0.11 * 100), rep(\"Asian\", 0.03 * 100))\n) |&gt; \n  pivot_longer(cols = everything(),\n               names_to = \"gender\", values_to = \"ethnic\") \n\nemployee\n\n# A tibble: 200 × 2\n   gender ethnic\n   &lt;chr&gt;  &lt;chr&gt; \n 1 male   White \n 2 female White \n 3 male   White \n 4 female White \n 5 male   White \n 6 female White \n 7 male   White \n 8 female White \n 9 male   White \n10 female White \n# ℹ 190 more rows\n\n\n\ntable(employee$gender, employee$ethnic)\n\n        \n         Am Asian Black White\n  female 11     3    21    65\n  male    1    14    25    60\n\n\n\nemployee_chisq &lt;- chisq.test(table(employee$gender, employee$ethnic))\nemployee_chisq\n\n\n    Pearson's Chi-squared test\n\ndata:  table(employee$gender, employee$ethnic)\nX-squared = 15.999, df = 3, p-value = 0.001135\n\n\n\\[\nw = \\sqrt{ \\frac{ \\chi_{2} }{ n \\times df }}\n\\]\nX2= Chi-squared = ∑(O-E)2/E\n\neff_size5 &lt;- sqrt( unname(employee_chisq$statistic) / (200 * (4 - 1)))\npwr.chisq.test(eff_size5,  df=3, sig.level=0.05, power=0.80)\n\n\n     Chi squared power calculation \n\n              w = 0.1632932\n              N = 408.8766\n             df = 3\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: N is the number of observations"
  },
  {
    "objectID": "stats/sam-size.html#simple-linear-reg",
    "href": "stats/sam-size.html#simple-linear-reg",
    "title": "Sample Size (Traditional)",
    "section": "Simple Linear Reg",
    "text": "Simple Linear Reg\n\nEx 1\nYou are interested in determining if height (meters) in plants can predict yield (grams of berries). You collect the following trial data.\n\nplant_yield &lt;- data.frame(\n  yield = c(46.8, 48.7, 48.4, 53.7, 56.7),\n  height = c(14.6, 19.6, 18.6, 25.5, 20.4)\n)\n\n\nplant_yield_lmsum &lt;- lm(height ~ yield, data = plant_yield) |&gt; summary()\nplant_yield_lmsum\n\n\nCall:\nlm(formula = height ~ yield, data = plant_yield)\n\nResiduals:\n     1      2      3      4      5 \n-2.554  1.236  0.427  3.951 -3.060 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -12.6564    20.3700  -0.621    0.578\nyield         0.6370     0.3994   1.595    0.209\n\nResidual standard error: 3.327 on 3 degrees of freedom\nMultiple R-squared:  0.4588,    Adjusted R-squared:  0.2784 \nF-statistic: 2.543 on 1 and 3 DF,  p-value: 0.2091\n\n\n\neff_size6 &lt;- sqrt(plant_yield_lmsum$adj.r.squared)\n\npwr.f2.test(u=1, f2=eff_size6, sig.level=0.05, power=0.80)\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 15.02932\n             f2 = 0.5275993\n      sig.level = 0.05\n          power = 0.8\n\n\n\nu = numerator degrees of freedom (n_vars - 1)\nv = denominator degrees of freedom\nf2 = effect size\n\nSample Size = v + n_vars\n\n# N\nceiling(15.02932) + 2\n\n[1] 18\n\n\n\n\nEx 2 (No Prior)\nYou are interested in determining if the size of a city (in square miles) can predict the population of the city (in # of individuals).\n\ncohen.ES(test = \"f2\", size = \"large\")\n\n\n     Conventional effect size from Cohen (1982) \n\n           test = f2\n           size = large\n    effect.size = 0.35\n\n\n\npwr.f2.test(u = 1, \n            f2 = 0.35,  \n            sig.level=0.05, power=0.80)\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 22.50313\n             f2 = 0.35\n      sig.level = 0.05\n          power = 0.8\n\n\n\n# N\nceiling(22.50313) + 2\n\n[1] 25"
  },
  {
    "objectID": "stats/sam-size.html#multiple-linear-reg",
    "href": "stats/sam-size.html#multiple-linear-reg",
    "title": "Sample Size (Traditional)",
    "section": "Multiple Linear Reg",
    "text": "Multiple Linear Reg\n\nEx 1\nYou are interested in determining if height (meters), weight (grams), and fertilizer added (grams) in plants can predict yield (grams of berries). You collect the following trial data.\n\nplant_yield2 &lt;- data.frame(\n  yield = c(46.8, 48.7, 48.4, 53.7, 56.7),\n  height = c(14.6, 19.6, 18.6, 25.5, 20.4),\n  weight = c(95.3, 99.5, 94.1, 110, 103),\n  Fert = c(2.1, 3.2, 4.3, 1.1, 4.3)\n)\n\n\nplant_yield2_lmsum &lt;- lm(height~yield + weight + Fert, \n                         data = plant_yield2) |&gt; summary()\nplant_yield2_lmsum\n\n\nCall:\nlm(formula = height ~ yield + weight + Fert, data = plant_yield2)\n\nResiduals:\n      1       2       3       4       5 \n-0.4799 -1.2588  1.5100  0.7626 -0.5340 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -64.2253    30.2429  -2.124    0.280\nyield        -0.7999     0.7948  -1.006    0.498\nweight        1.1779     0.5988   1.967    0.299\nFert          2.1379     1.8204   1.174    0.449\n\nResidual standard error: 2.227 on 1 degrees of freedom\nMultiple R-squared:  0.9191,    Adjusted R-squared:  0.6765 \nF-statistic: 3.788 on 3 and 1 DF,  p-value: 0.3571\n\n\n\neff_size7 &lt;- sqrt(plant_yield2_lmsum$adj.r.squared)\n\npwr.f2.test(u=3, f2=eff_size7, sig.level=0.05, power=0.80)\n\n\n     Multiple regression power calculation \n\n              u = 3\n              v = 13.69382\n             f2 = 0.8225024\n      sig.level = 0.05\n          power = 0.8\n\n\n\n# N\nceiling(13.69382) + 4\n\n[1] 18\n\n\n\n\nEx 2 (No Prior)\nYou are interested in determining if the size of a city (in square miles), number of houses, number of apartments, and number of jobs can predict the population of the city (in # of individuals)\n\ncohen.ES(test = \"f2\", size = \"large\")\n\n\n     Conventional effect size from Cohen (1982) \n\n           test = f2\n           size = large\n    effect.size = 0.35\n\n\n\npwr.f2.test(u = 3, # 4 Variables\n            f2 = 0.35,  \n            sig.level=0.05, power=0.80)\n\n\n     Multiple regression power calculation \n\n              u = 3\n              v = 31.3129\n             f2 = 0.35\n      sig.level = 0.05\n          power = 0.8\n\n\n\n# N\nceiling(31.3129) + 4\n\n[1] 36"
  },
  {
    "objectID": "stats/sam-size.html#logistic-reg",
    "href": "stats/sam-size.html#logistic-reg",
    "title": "Sample Size (Traditional)",
    "section": "Logistic Reg",
    "text": "Logistic Reg\n\np0 = \\(Prob(Y=1|X=0)\\): the probability of observing 1 for the outcome variable Y when the predictor X equals 0\np1 = \\(Prob(Y=1|X=1)\\): the probability of observing 1 for the outcome variable Y when the predictor X equals 1\n\nIn the context of using the wp.logistic() function from the “WebPower” package in R to calculate the sample size for a logistic regression, the arguments p0 and p1 are crucial. They represent the probabilities of the outcome occurring in the two groups you are comparing. Here’s how to determine these values:\n\nUnderstanding p0 and p1:\n\np0: This is the probability of the event (or the success probability) in the control group or the group without the intervention/exposure.\np1: This is the probability of the event in the experimental group or the group with the intervention/exposure.\n\nObtaining p0 and p1:\n\nThese probabilities are usually obtained from prior research, pilot studies, or literature reviews. You need an estimate of how likely the event is in both the control and experimental groups.\nIf you’re testing a new treatment or intervention, p1 would be your expected success rate with the treatment, and p0 would be the success rate observed in the control group or with the standard treatment.\nIn the absence of prior data, expert opinion or theoretical assumptions might be used to estimate these probabilities.\n\nExample:\n\nSuppose you are studying a new medication’s effect on reducing the incidence of a disease. From previous studies, you know that 20% of patients (0.20 probability) typically show improvement with the current standard medication (p0). You expect that 35% of patients (0.35 probability) will show improvement with the new medication (p1).\n\n\n\nEx 1\nYou are interested in determining if body temperature influences sleep disorder prevalence (yes 1, no 0). You collect the following trial data.\n\nsleep_temp &lt;- data.frame(\n  temp = c(98.6, 98.5, 99, 97.5, 98.8, 98.2, 98.5, 98.4, 98.1), \n  sleep_disorder = c(\"No\", \"No\", \"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"Yes\", \"No\")\n)\n\n\nshapiro.test(sleep_temp$temp) # Normal\n\n\n    Shapiro-Wilk normality test\n\ndata:  sleep_temp$temp\nW = 0.94543, p-value = 0.6401\n\n\n\nsleep_temp |&gt; \n  ggplot(aes(sleep_disorder, temp, color = sleep_disorder)) +\n  geom_boxplot()\n\n\n\n  geom_point()\n\ngeom_point: na.rm = FALSE\nstat_identity: na.rm = FALSE\nposition_identity \n\n\n\nwp.logistic(p0=0.33, p1=0.67, # Why ????\n            alpha=0.05, power=0.80, \n            alternative=\"two.sided\", family=\"normal\")\n\nPower for logistic regression\n\n      p0   p1      beta0   beta1        n alpha power\n    0.33 0.67 -0.7081851 1.41637 40.79646  0.05   0.8\n\nURL: http://psychstat.org/logistic"
  },
  {
    "objectID": "stats/causal-dag.html#fork-confounder",
    "href": "stats/causal-dag.html#fork-confounder",
    "title": "Causal Inference (DAG)",
    "section": "Fork (Confounder)",
    "text": "Fork (Confounder)\n\nSim Data\n\nset.seed(123)\nn &lt;- 1000\n\nq &lt;- rbinom(n, size = 1, prob = .35)\nx &lt;- 2 * q + rnorm(n)\ny &lt;- -3 * q + rnorm(n)\n\nconfounder_data &lt;- tibble(x, y, q = as.factor(q))\n\n\n\nPlot\n\n\nCode\np1 &lt;- confounder_data |&gt;\n  ggplot(aes(x, y)) +\n  geom_point(alpha = .2) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n  facet_wrap(~\"not adjusting for `q`\\n(biased)\")\n\np2 &lt;- confounder_data |&gt;\n  ggplot(aes(x, y, color = q)) +\n  geom_point(alpha = .2) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~\"adjusting for `q`\\n(unbiased)\")\n\np1 + p2\n\n\n\n\n\n\n\nCorr\n\nlares::corr_cross(confounder_data)\n\n\n\n\n\ncorrelation(confounder_data)\n\n# Correlation Matrix (pearson-method)\n\nParameter1 | Parameter2 |     r |         95% CI | t(998) |         p\n---------------------------------------------------------------------\nx          |          y | -0.54 | [-0.58, -0.49] | -20.26 | &lt; .001***\n\np-value adjustment method: Holm (1979)\nObservations: 1000\n\n\n\n\nModel\n\n# Not Adjust\n## y ~ x\nconfounder_data.fit.y_x &lt;- lm(y ~ x, data = confounder_data)\n## y ~ q \nconfounder_data.fit.y_q &lt;- lm(y ~ q, data = confounder_data)\n## x ~ q\nconfounder_data.fit.x_q &lt;-  lm(x ~ q, data = confounder_data)\n\n# Adjusted: y ~ x + q \nconfounder_data.fit.y_xq &lt;- lm(y ~ x + q, data = confounder_data)\n\n\n\nCode\nconfounder_data.fit.tbls &lt;- \n  list(\n    y_x = confounder_data.fit.y_x,\n    y_q = confounder_data.fit.y_q,\n    x_q = confounder_data.fit.x_q,\n    y_xq = confounder_data.fit.y_xq\n  ) |&gt; \n  map(tbl_regression_custom)\n  \nconfounder_data.fit.tbls |&gt; \ntbl_merge(\n  tab_spanner = c(\"**y ~ x**\", \"**y ~ q**\", \"**x ~ q**\", \"**y ~ x + q**\")\n  )\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      \n        y ~ x\n      \n      \n        y ~ q\n      \n      \n        x ~ q\n      \n      \n        y ~ x + q\n      \n    \n    \n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    x\n-0.70\n-0.77, -0.63\n&lt;0.001\n\n\n\n\n\n\n0.06\n0.00, 0.12\n0.059\n    q\n\n\n\n\n\n\n\n\n\n\n\n\n        0\n\n\n\n—\n—\n\n—\n—\n\n—\n—\n\n        1\n\n\n\n-3.1\n-3.2, -3.0\n&lt;0.001\n1.9\n1.8, 2.1\n&lt;0.001\n-3.2\n-3.4, -3.0\n&lt;0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval"
  },
  {
    "objectID": "stats/causal-dag.html#chain-mediator",
    "href": "stats/causal-dag.html#chain-mediator",
    "title": "Causal Inference (DAG)",
    "section": "Chain (Mediator)",
    "text": "Chain (Mediator)\n\nSim Data\n\nset.seed(123)\nx &lt;- rnorm(n)\n\nlinear_pred &lt;- 2 * x + rnorm(n)\nprob &lt;- 1 / (1 + exp(-linear_pred))\nq &lt;- rbinom(n, size = 1, prob = prob)\ny &lt;- 2 * q + rnorm(n)\n\nmediator_data &lt;- tibble(x, y, q = as.factor(q))\n\n\n\nPlot\n\n\nCode\np1 &lt;- mediator_data |&gt;\n  ggplot(aes(x, y)) +\n  geom_point(alpha = .2) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n  facet_wrap(~\"not adjusting for `q`\\n(total effect)\")\n\np2 &lt;- mediator_data |&gt;\n  ggplot(aes(x, y, color = q)) +\n  geom_point(alpha = .2) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~\"adjusting for `q`\\n(direct effect)\")\n\np1 + p2\n\n\n\n\n\n\n\nCorr\n\nlares::corr_cross(mediator_data)\n\n\n\n\n\ncorrelation(mediator_data)\n\n# Correlation Matrix (pearson-method)\n\nParameter1 | Parameter2 |    r |       95% CI | t(998) |         p\n------------------------------------------------------------------\nx          |          y | 0.41 | [0.35, 0.46] |  14.04 | &lt; .001***\n\np-value adjustment method: Holm (1979)\nObservations: 1000\n\n\n\n\nModel\n\n# Not Adjust\n## y ~ x\nmediator_data.fit.y_x &lt;- lm(y ~ x, data = mediator_data)\n## y ~ q \nmediator_data.fit.y_q &lt;- lm(y ~ q, data = mediator_data)\n## x ~ q\nmediator_data.fit.x_q &lt;-  lm(x ~ q, data = mediator_data)\n\n# Adjusted: y ~ x + q \nmediator_data.fit.y_xq &lt;- lm(y ~ x + q, data = mediator_data)\n\n\n\nCode\nmediator_data.fit.tbls &lt;- \n  list(\n    y_x = mediator_data.fit.y_x,\n    y_q = mediator_data.fit.y_q,\n    x_q = mediator_data.fit.x_q,\n    y_xq = mediator_data.fit.y_xq\n  ) |&gt; \n  map(tbl_regression_custom)\n  \nmediator_data.fit.tbls |&gt; \ntbl_merge(\n  tab_spanner = c(\"**y ~ x**\", \"**y ~ q**\", \"**x ~ q**\", \"**y ~ x + q**\")\n  )\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      \n        y ~ x\n      \n      \n        y ~ q\n      \n      \n        x ~ q\n      \n      \n        y ~ x + q\n      \n    \n    \n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    x\n0.57\n0.49, 0.65\n&lt;0.001\n\n\n\n\n\n\n-0.02\n-0.10, 0.06\n0.622\n    q\n\n\n\n\n\n\n\n\n\n\n\n\n        0\n\n\n\n—\n—\n\n—\n—\n\n—\n—\n\n        1\n\n\n\n2.0\n1.8, 2.1\n&lt;0.001\n1.2\n1.1, 1.3\n&lt;0.001\n2.0\n1.8, 2.1\n&lt;0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval"
  },
  {
    "objectID": "stats/causal-dag.html#collider-bias",
    "href": "stats/causal-dag.html#collider-bias",
    "title": "Causal Inference (DAG)",
    "section": "Collider (Bias)",
    "text": "Collider (Bias)\n\nSim Data\n\nset.seed(1)\nx &lt;- rnorm(n)\ny &lt;- rnorm(n)\n\nlinear_pred &lt;- 2 * x + 3 * y + rnorm(n)\nprob &lt;- 1 / (1 + exp(-linear_pred))\nq &lt;- rbinom(n, size = 1, prob = prob)\n\ncollider_data &lt;- tibble(x, y, q = as.factor(q))\n\n\n\nPlot\n\n\nCode\np1 &lt;- collider_data |&gt;\n  ggplot(aes(x, y)) +\n  geom_point(alpha = .2) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n  facet_wrap(~\"not adjusting for `q`\\n(unbiased)\")\n\np2 &lt;- collider_data |&gt;\n  ggplot(aes(x, y, color = q)) +\n  geom_point(alpha = .2) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~\"adjusting for `q`\\n(biased)\")\n\np1 + p2\n\n\n\n\n\n\n\nCorr\n\nlares::corr_cross(collider_data)\n\n\n\n\n\ncorrelation(collider_data)\n\n# Correlation Matrix (pearson-method)\n\nParameter1 | Parameter2 |        r |        95% CI | t(998) |     p\n-------------------------------------------------------------------\nx          |          y | 6.40e-03 | [-0.06, 0.07] |   0.20 | 0.840\n\np-value adjustment method: Holm (1979)\nObservations: 1000\n\n\n\n\nModel\n\n# Not Adjust\n## y ~ x\ncollider_data.fit.y_x &lt;- lm(y ~ x, data = collider_data)\n## y ~ q \ncollider_data.fit.y_q &lt;- lm(y ~ q, data = collider_data)\n## x ~ q\ncollider_data.fit.x_q &lt;-  lm(x ~ q, data = collider_data)\n\n# Bias: y ~ x + q \ncollider_data.fit.y_xq &lt;- lm(y ~ x + q, data = collider_data)\n\n\n\nCode\ncollider_data.fit.tbls &lt;- \n  list(\n    y_x = collider_data.fit.y_x,\n    y_q = collider_data.fit.y_q,\n    x_q = collider_data.fit.x_q,\n    y_xq = collider_data.fit.y_xq\n  ) |&gt; \n  map(tbl_regression_custom)\n  \ncollider_data.fit.tbls |&gt; \ntbl_merge(\n  tab_spanner = c(\"**y ~ x**\", \"**y ~ q**\", \"**x ~ q**\", \"**Bias: y ~ x + q**\")\n  )\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      \n        y ~ x\n      \n      \n        y ~ q\n      \n      \n        x ~ q\n      \n      \n        Bias: y ~ x + q\n      \n    \n    \n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    x\n0.01\n-0.06, 0.07\n0.840\n\n\n\n\n\n\n-0.25\n-0.30, -0.20\n&lt;0.001\n    q\n\n\n\n\n\n\n\n\n\n\n\n\n        0\n\n\n\n—\n—\n\n—\n—\n\n—\n—\n\n        1\n\n\n\n1.2\n1.1, 1.3\n&lt;0.001\n0.79\n0.67, 0.91\n&lt;0.001\n1.4\n1.3, 1.5\n&lt;0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval"
  },
  {
    "objectID": "stats/causal-malaria.html#explore",
    "href": "stats/causal-malaria.html#explore",
    "title": "Causal Inference (Malaria)",
    "section": "Explore",
    "text": "Explore\nExposure: net\nOutcome: malaria_risk\n\nnames(net_data)\n\n [1] \"id\"                     \"net\"                    \"net_num\"               \n [4] \"malaria_risk\"           \"income\"                 \"health\"                \n [7] \"household\"              \"eligible\"               \"temperature\"           \n[10] \"insecticide_resistance\"\n\n\n\nskimr::skim(net_data)\n\n\nData summary\n\n\nName\nnet_data\n\n\nNumber of rows\n1752\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nlogical\n2\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: logical\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nnet\n0\n1\n0.26\nFAL: 1298, TRU: 454\n\n\neligible\n0\n1\n0.02\nFAL: 1716, TRU: 36\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nid\n0\n1\n876.50\n505.90\n1.0\n438.75\n876.5\n1314.25\n1752.0\n▇▇▇▇▇\n\n\nnet_num\n0\n1\n0.26\n0.44\n0.0\n0.00\n0.0\n1.00\n1.0\n▇▁▁▁▃\n\n\nmalaria_risk\n0\n1\n39.67\n15.37\n10.0\n28.00\n36.0\n50.00\n90.0\n▃▇▅▂▁\n\n\nincome\n0\n1\n897.76\n191.24\n330.0\n764.75\n893.0\n1031.00\n1484.0\n▁▅▇▅▁\n\n\nhealth\n0\n1\n50.21\n19.35\n5.0\n37.00\n50.0\n64.00\n100.0\n▂▆▇▅▁\n\n\nhousehold\n0\n1\n2.97\n1.41\n1.0\n2.00\n3.0\n4.00\n9.0\n▇▇▂▁▁\n\n\ntemperature\n0\n1\n23.93\n4.10\n15.6\n20.90\n23.8\n27.10\n32.2\n▃▇▇▇▃\n\n\ninsecticide_resistance\n0\n1\n50.08\n14.44\n5.0\n40.00\n50.0\n60.00\n95.0\n▁▅▇▃▁\n\n\n\n\n\n\nnet_data_dep_vars &lt;- net_data |&gt; \n  select(!c(malaria_risk, id, net_num)) |&gt; names()"
  },
  {
    "objectID": "stats/causal-malaria.html#true-relationship",
    "href": "stats/causal-malaria.html#true-relationship",
    "title": "Causal Inference (Malaria)",
    "section": "True Relationship",
    "text": "True Relationship\n\n\n\nActual relationship between variables"
  },
  {
    "objectID": "stats/causal-malaria.html#correlation",
    "href": "stats/causal-malaria.html#correlation",
    "title": "Causal Inference (Malaria)",
    "section": "Correlation",
    "text": "Correlation\n\n\n\nSignificant correlation path between variables\n\n\n\nnet_data_cor &lt;- net_data |&gt; \n  corrr::correlate(quiet = T) \n\n\nCorr MatrixNetwork\n\n\n\ncorrr::rplot(shave(net_data_cor),  print_cor = TRUE)\n\n\n\n\n\n\n\nnetwork_plot(net_data_cor)"
  },
  {
    "objectID": "stats/causal-malaria.html#outcome-malaria_risk-lm",
    "href": "stats/causal-malaria.html#outcome-malaria_risk-lm",
    "title": "Causal Inference (Malaria)",
    "section": "Outcome: malaria_risk (LM)",
    "text": "Outcome: malaria_risk (LM)\n\nUnivar LM\n\n\n\nSignificant variable of univariate linear regression with outcome = malaria_risk\n\n\n\n\nCode\nnet_data_tbl.malaria.uv &lt;- net_data |&gt;\n  select(all_of(net_data_dep_vars), malaria_risk) |&gt; \n  tbl_uvregression(\n    method = lm,\n    y = malaria_risk,\n    pvalue_fun = ~ style_pvalue(.x, digits = 3)\n  ) |&gt; \n  bold_p() |&gt; \n  bold_labels()\n\nnet_data_tbl.malaria.uv\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      N\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    net\n1,752\n\n\n\n        FALSE\n\n—\n—\n\n        TRUE\n\n-16\n-18, -15\n&lt;0.001\n    income\n1,752\n-0.06\n-0.07, -0.06\n&lt;0.001\n    health\n1,752\n-0.45\n-0.48, -0.41\n&lt;0.001\n    household\n1,752\n-0.09\n-0.60, 0.43\n0.740\n    eligible\n1,752\n\n\n\n        FALSE\n\n—\n—\n\n        TRUE\n\n20\n15, 25\n&lt;0.001\n    temperature\n1,752\n0.62\n0.45, 0.79\n&lt;0.001\n    insecticide_resistance\n1,752\n0.21\n0.16, 0.25\n&lt;0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\n\n\nMultivar LM\n\n\n\nSignificant predictors using simple LM & best subset predictors using multivariate LM with outcome = malaria_risk\n\n\n\nUsing best subset with leaps\n\nlibrary(leaps)\nsource(here(\"R/leaps-extra.R\"))\n\n\nnet_data_mod &lt;- net_data |&gt; select(all_of(net_data_dep_vars), malaria_risk)\n\n\nnet_data_malaria_rss.fit &lt;- leaps::regsubsets(\n  malaria_risk ~ ., \n  data = net_data_mod, \n  force.in = 1 # Force to include `net` as predictors\n  )\n\n# broom::tidy(net_data_malaria_rss.fit) \n\n\n\nCode\nlibrary(patchwork)\nlibrary(latex2exp)\n\np_cp &lt;- autoplot(net_data_malaria_rss.fit, res = \"mallows_cp\") +\n  labs(y = TeX(\"$C_p$\"))\n\np_bic &lt;- autoplot(net_data_malaria_rss.fit, res = \"BIC\")\n\np_adj_rsq &lt;- autoplot(net_data_malaria_rss.fit, res = \"adj.r.squared\") +\n  labs(y = TeX(\"Adjusted $R^2$\"))\n\np_cp + p_bic + p_adj_rsq +\n  plot_annotation(title = \"Best Subset Selection at Each Model Sizes\", \n                  subtitle = TeX(\"Estimate test error by $C_p$, BIC, and Adjusted $R^2$\"))\n\n\n\n\n\n\nplot(net_data_malaria_rss.fit, scale = \"bic\")\n\n\n\n\n\n\nFit Best Multi LM\n\nnet_data_malaria_lm.fit &lt;- lm(\n  malaria_risk ~ net + income + health + temperature + insecticide_resistance, \n  data = net_data_mod\n  )\n\n\n\nCode\nnet_data_tbl.malaria.mv &lt;- net_data_malaria_lm.fit |&gt; \n  tbl_regression(\n    pvalue_fun = ~ style_pvalue(.x, digits = 3)\n    ) |&gt; \n  bold_p() |&gt; \n  bold_labels()\n\nnet_data_tbl.malaria.mv\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    net\n\n\n\n        FALSE\n—\n—\n\n        TRUE\n-12\n-13, -12\n&lt;0.001\n    income\n-0.08\n-0.08, -0.07\n&lt;0.001\n    health\n0.14\n0.12, 0.15\n&lt;0.001\n    temperature\n1.0\n0.98, 1.1\n&lt;0.001\n    insecticide_resistance\n0.22\n0.20, 0.23\n&lt;0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\n\n\n\nSummary\nOutcome = malaria_risk\n\n\n\nSignificant predictors using simple LM & best subset predictors using multivariate LM with outcome = malaria_risk\n\n\n\n\nCode\nnet_data_tbl.malaria.uv.mv &lt;- tbl_merge(\n  list(net_data_tbl.malaria.uv, net_data_tbl.malaria.mv),\n  tab_spanner = c(\"**Univar**\", \"**Multivar**\")\n  )\n\nnet_data_tbl.malaria.uv.mv\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      \n        Univar\n      \n      \n        Multivar\n      \n    \n    \n      N\n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    net\n1,752\n\n\n\n\n\n\n        FALSE\n\n—\n—\n\n—\n—\n\n        TRUE\n\n-16\n-18, -15\n&lt;0.001\n-12\n-13, -12\n&lt;0.001\n    income\n1,752\n-0.06\n-0.07, -0.06\n&lt;0.001\n-0.08\n-0.08, -0.07\n&lt;0.001\n    health\n1,752\n-0.45\n-0.48, -0.41\n&lt;0.001\n0.14\n0.12, 0.15\n&lt;0.001\n    household\n1,752\n-0.09\n-0.60, 0.43\n0.740\n\n\n\n    eligible\n1,752\n\n\n\n\n\n\n        FALSE\n\n—\n—\n\n\n\n\n        TRUE\n\n20\n15, 25\n&lt;0.001\n\n\n\n    temperature\n1,752\n0.62\n0.45, 0.79\n&lt;0.001\n1.0\n0.98, 1.1\n&lt;0.001\n    insecticide_resistance\n1,752\n0.21\n0.16, 0.25\n&lt;0.001\n0.22\n0.20, 0.23\n&lt;0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval"
  },
  {
    "objectID": "stats/causal-malaria.html#outcome-net-lr",
    "href": "stats/causal-malaria.html#outcome-net-lr",
    "title": "Causal Inference (Malaria)",
    "section": "Outcome: net (LR)",
    "text": "Outcome: net (LR)\n\nUnivar LogReg\n\n\n\nSignificant variable of univariate logistic regression with outcome = net\n\n\n\n\nCode\nnet_data |&gt;\n  select(all_of(net_data_dep_vars), malaria_risk) |&gt; \n  tbl_uvregression(\n    method = glm,\n    y = net,\n    method.args = list(family = binomial),\n    exponentiate = TRUE,\n    pvalue_fun = ~ style_pvalue(.x, digits = 3)\n  ) |&gt; \n  bold_p() |&gt; \n  bold_labels()\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      N\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    income\n1,752\n1.00\n1.00, 1.00\n&lt;0.001\n    health\n1,752\n1.01\n1.01, 1.02\n&lt;0.001\n    household\n1,752\n1.09\n1.01, 1.17\n0.029\n    eligible\n1,752\n\n\n\n        FALSE\n\n—\n—\n\n        TRUE\n\n2.94\n1.51, 5.73\n0.001\n    temperature\n1,752\n0.98\n0.96, 1.01\n0.163\n    insecticide_resistance\n1,752\n1.00\n1.00, 1.01\n0.360\n    malaria_risk\n1,752\n0.88\n0.87, 0.90\n&lt;0.001\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\n\n\nMultivar Logreg\n\n\n\nSignificant predictors using simple LR & best subset predictors using multivariate LR with outcome = net\n\n\n\nlibrary(bestglm)\n\n\nnet_data_Xy_net &lt;- net_data_mod |&gt; \n  relocate(net, .after = last_col()) |&gt; \n  as.data.frame()\n\n\nnet_data_net.bestglm &lt;- bestglm::bestglm(net_data_Xy_net, \n                                         family = binomial(), \n                                         IC = \"BIC\")\n\nMorgan-Tatar search since family is non-gaussian.\n\nnet_data_net.bestglm$BestModels\n\n  income health household eligible temperature insecticide_resistance\n1   TRUE   TRUE     FALSE     TRUE        TRUE                   TRUE\n2   TRUE   TRUE      TRUE     TRUE        TRUE                   TRUE\n3   TRUE  FALSE     FALSE     TRUE        TRUE                   TRUE\n4   TRUE  FALSE      TRUE     TRUE        TRUE                   TRUE\n5   TRUE   TRUE     FALSE    FALSE        TRUE                   TRUE\n  malaria_risk Criterion\n1         TRUE  642.8398\n2         TRUE  650.3082\n3         TRUE  675.0623\n4         TRUE  682.5247\n5         TRUE  698.3880"
  },
  {
    "objectID": "stats/causal-malaria.html#summary-1",
    "href": "stats/causal-malaria.html#summary-1",
    "title": "Causal Inference (Malaria)",
    "section": "Summary",
    "text": "Summary\n\n\n\nSummary of univariate & multivariate analysis: correlation and regression with outcome as malaria_risk or net\n\n\n\n\nCode\nnet_data_tbl.malaria.uv.mv\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      \n        Univar\n      \n      \n        Multivar\n      \n    \n    \n      N\n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    net\n1,752\n\n\n\n\n\n\n        FALSE\n\n—\n—\n\n—\n—\n\n        TRUE\n\n-16\n-18, -15\n&lt;0.001\n-12\n-13, -12\n&lt;0.001\n    income\n1,752\n-0.06\n-0.07, -0.06\n&lt;0.001\n-0.08\n-0.08, -0.07\n&lt;0.001\n    health\n1,752\n-0.45\n-0.48, -0.41\n&lt;0.001\n0.14\n0.12, 0.15\n&lt;0.001\n    household\n1,752\n-0.09\n-0.60, 0.43\n0.740\n\n\n\n    eligible\n1,752\n\n\n\n\n\n\n        FALSE\n\n—\n—\n\n\n\n\n        TRUE\n\n20\n15, 25\n&lt;0.001\n\n\n\n    temperature\n1,752\n0.62\n0.45, 0.79\n&lt;0.001\n1.0\n0.98, 1.1\n&lt;0.001\n    insecticide_resistance\n1,752\n0.21\n0.16, 0.25\n&lt;0.001\n0.22\n0.20, 0.23\n&lt;0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval"
  },
  {
    "objectID": "stats/dta.html#survived-vs-fare-any-difference",
    "href": "stats/dta.html#survived-vs-fare-any-difference",
    "title": "Diagnosis Accuracy",
    "section": "Survived vs Fare: Any difference ?",
    "text": "Survived vs Fare: Any difference ?\nLet’s ask the following question: were those people who paid more for their ticket more likely to survive?\n\nggstatsplot::ggbetweenstats(\n  titanic_train,\n  x = Survived,\n  y = Fare\n)\n\n\n\n\nConfirm the difference in Fare between 2 groups.\n\nwilcox.test(titanic_train$Fare ~ titanic_train$Survived)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  titanic_train$Fare by titanic_train$Survived\nW = 57806, p-value &lt; 2.2e-16\nalternative hypothesis: true location shift is not equal to 0"
  },
  {
    "objectID": "stats/dta.html#dta-manual-way",
    "href": "stats/dta.html#dta-manual-way",
    "title": "Diagnosis Accuracy",
    "section": "DTA (Manual Way)",
    "text": "DTA (Manual Way)\nNow let’s ask a slightly different question: can a passenger’s fare price be used to predict where or not they survived?\n\nPrep Data\n\ntitanic_sub &lt;- titanic_train |&gt; \n  select(Fare, Survived) |&gt; \n  mutate(Survived_orig = ifelse(Survived == 1L, \"Lived\", \"Died\")) |&gt; \n  mutate(Survived_pred = ifelse(Fare &gt; 14.45, \"Lived\", \"Died\")) |&gt; \n  mutate(across(starts_with(\"Survived_\"), \n                ~factor(.x, levels = c(\"Lived\", \"Died\"))))\n\nhead(titanic_sub)\n\n     Fare Survived Survived_orig Survived_pred\n1  7.2500        0          Died          Died\n2 71.2833        1         Lived         Lived\n3  7.9250        1         Lived          Died\n4 53.1000        1         Lived         Lived\n5  8.0500        0          Died          Died\n6  8.4583        0          Died          Died\n\n\n\n\nConfusion Matrix\n\ncm &lt;- table(pred = titanic_sub$Survived_pred, \n      orig = titanic_sub$Survived_orig)\n\ncm\n\n       orig\npred    Lived Died\n  Lived   231  220\n  Died    111  329\n\n\n\n\nDiagnostic Accuracy\n\n# True Positive\n(tp &lt;- cm[1, 1])\n\n[1] 231\n\n# False Positive\n(fp &lt;- cm[1, 2])\n\n[1] 220\n\n# False Negative\n(fn &lt;- cm[2, 1])\n\n[1] 111\n\n# True Negative\n(tn &lt;- cm[2, 2])\n\n[1] 329\n\n\n\n# Sense\ntp / (tp + fn)\n\n[1] 0.6754386\n\n# Spec\ntn / (tn + fp)\n\n[1] 0.5992714\n\n# PPV\ntp / (tp + fp)\n\n[1] 0.5121951\n\n# NPV\ntn / (tn + fn)\n\n[1] 0.7477273"
  },
  {
    "objectID": "stats/dta.html#roc-curve",
    "href": "stats/dta.html#roc-curve",
    "title": "Diagnosis Accuracy",
    "section": "ROC Curve",
    "text": "ROC Curve\n\nr1 &lt;- roc(Survived_orig ~ Fare, data = titanic_sub)\n\nSetting levels: control = Lived, case = Died\n\n\nSetting direction: controls &gt; cases\n\nr1\n\n\nCall:\nroc.formula(formula = Survived_orig ~ Fare, data = titanic_sub)\n\nData: Fare in 342 controls (Survived_orig Lived) &gt; 549 cases (Survived_orig Died).\nArea under the curve: 0.6921\n\n\n\n# AUC\n(auc &lt;- auc(r1))\n\nArea under the curve: 0.6921\n\n# Confidence Interval\n(ci &lt;- ci.auc(r1))\n\n95% CI: 0.6567-0.7276 (DeLong)\n\nci_l &lt;- round(ci[1], 2) # Lower\nci_u &lt;- round(ci[3], 2) # Upper\n\n\nhead(r1$thresholds)\n\n[1]      Inf 387.6646 262.6875 254.9479 237.5229 224.6521\n\n\n\nPlot (Base R)\n\nplot(r1, type = \"S\")\n\n\n\n\n\n\nPlot (ggplot2)\n\nlegend_text &lt;- paste0(\n    \"AUC = \", round(auc, 2), \" (95% CI = \", ci_l, \" - \", ci_u, \")\"\n)\n\n\nggroc(r1)+ \n  ggtitle(\"Receiver Operating Characteristic Curve\") +\n  geom_segment(\n    aes(x = 1, xend = 0, y = 0, yend = 1), color = \"grey\", \n    linetype = \"dashed\" ) +\n  scale_y_continuous(expand = c(0, 0)) + \n  scale_x_reverse(expand = c(0, 0)) + \n  annotate(\"text\", x = 0.3, y = 0.05, label = legend_text)\n\nScale for x is already present.\nAdding another scale for x, which will replace the existing scale."
  },
  {
    "objectID": "stats/dta-yardstick.html#confusion-matric",
    "href": "stats/dta-yardstick.html#confusion-matric",
    "title": "DTA (Yardstick)",
    "section": "Confusion Matric",
    "text": "Confusion Matric\n\ncm &lt;- conf_mat(pathology, truth = pathology, estimate = scan) \ncm\n#&gt;           Truth\n#&gt; Prediction abnorm norm\n#&gt;     abnorm    231   32\n#&gt;     norm       27   54\n\n\nsummary(cm)\n#&gt; # A tibble: 13 × 3\n#&gt;    .metric              .estimator .estimate\n#&gt;    &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n#&gt;  1 accuracy             binary         0.828\n#&gt;  2 kap                  binary         0.534\n#&gt;  3 sens                 binary         0.895\n#&gt;  4 spec                 binary         0.628\n#&gt;  5 ppv                  binary         0.878\n#&gt;  6 npv                  binary         0.667\n#&gt;  7 mcc                  binary         0.534\n#&gt;  8 j_index              binary         0.523\n#&gt;  9 bal_accuracy         binary         0.762\n#&gt; 10 detection_prevalence binary         0.765\n#&gt; 11 precision            binary         0.878\n#&gt; 12 recall               binary         0.895\n#&gt; 13 f_meas               binary         0.887"
  },
  {
    "objectID": "stats/dta-yardstick.html#plot-bar-chart",
    "href": "stats/dta-yardstick.html#plot-bar-chart",
    "title": "DTA (Yardstick)",
    "section": "Plot Bar Chart",
    "text": "Plot Bar Chart\n\nautoplot(cm, type = \"mosaic\")\n\n\n\n\n\nautoplot(cm, type = \"heatmap\")\n\n\n\n\n\npathology_cell &lt;- pathology |&gt; \n  count(pathology, scan) |&gt; \n  mutate(prop = n/sum(n))\n\npathology_cell\n#&gt;   pathology   scan   n       prop\n#&gt; 1    abnorm abnorm 231 0.67151163\n#&gt; 2    abnorm   norm  27 0.07848837\n#&gt; 3      norm abnorm  32 0.09302326\n#&gt; 4      norm   norm  54 0.15697674\n\n\npathology_cell |&gt; \n  ggplot(aes(pathology, prop, fill = scan, color = scan)) +\n  geom_col(alpha = 0.5, position = \"fill\")"
  },
  {
    "objectID": "stats/dta-yardstick.html#metric-default",
    "href": "stats/dta-yardstick.html#metric-default",
    "title": "DTA (Yardstick)",
    "section": "Metric: Default",
    "text": "Metric: Default\n\npathology |&gt; metrics(truth = pathology, estimate = scan)\n#&gt; # A tibble: 2 × 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.828\n#&gt; 2 kap      binary         0.534"
  },
  {
    "objectID": "stats/dta-yardstick.html#metric-set",
    "href": "stats/dta-yardstick.html#metric-set",
    "title": "DTA (Yardstick)",
    "section": "Metric Set",
    "text": "Metric Set\n\nclass_metrics_1 &lt;- metric_set(accuracy, sens, spec, ppv, npv)\n\n\npathology |&gt; class_metrics_1(truth = pathology, estimate = scan)\n#&gt; # A tibble: 5 × 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.828\n#&gt; 2 sens     binary         0.895\n#&gt; 3 spec     binary         0.628\n#&gt; 4 ppv      binary         0.878\n#&gt; 5 npv      binary         0.667"
  },
  {
    "objectID": "stats/dta-yardstick.html#custom-class-metric",
    "href": "stats/dta-yardstick.html#custom-class-metric",
    "title": "DTA (Yardstick)",
    "section": "Custom Class Metric",
    "text": "Custom Class Metric\nMiss Rate Example"
  },
  {
    "objectID": "stats/dta-yardstick.html#custom-lr-lr-",
    "href": "stats/dta-yardstick.html#custom-lr-lr-",
    "title": "DTA (Yardstick)",
    "section": "Custom LR+ & LR-",
    "text": "Custom LR+ & LR-\nFrom: How to implemen custom metric set\n\nFormular\nThe positive likelihood ratio is calculated as:\n\\[\n{\\displaystyle {\\text{LR}}+={\\frac {\\text{sensitivity}}{1-{\\text{specificity}}}}}\n\\]\n\\[\n{\\displaystyle {\\text{LR}}+={\\frac {\\text{TP / (TP + FN)}}{\\text{FP / (FP + TN)}}}}\n\\]\nThe negative likelihood ratio is calculated as:\n\\[\n{\\displaystyle {\\text{LR}}-={\\frac {1-{\\text{sensitivity}}}{\\text{specificity}}}}\n\\]\n\\[\n{\\displaystyle {\\text{LR}}-={\\frac {\\text{FN / (TP + FN)}}{\\text{TN / (FP + TN)}}}}\n\\]\n\n\nExample\n\npathology_xtab &lt;- table(pathology$scan, pathology$pathology) \npathology_xtab\n#&gt;         \n#&gt;          abnorm norm\n#&gt;   abnorm    231   32\n#&gt;   norm       27   54\n\n\n\nHelpers\n\n# Logic for `event_level`\nevent_col &lt;- function(xtab, event_level) {\n  if (identical(event_level, \"first\")) {\n    colnames(xtab)[[1]]\n  } else {\n    colnames(xtab)[[2]]\n  }\n}\n\n\nfinalize_estimator_internal.lr_pos &lt;- function(metric_dispatcher, x, estimator, call) {\n  \n  validate_estimator(estimator, estimator_override = \"binary\")\n  if (!is.null(estimator)) {\n    return(estimator)\n  }\n  \n  lvls &lt;- levels(x)\n  if (length(lvls) &gt; 2) {\n    stop(\"A multiclass `truth` input was provided, but only `binary` is supported.\")\n  } \n  \"binary\"\n}\n\n\n\nImplement\n\nLR Pos\n\nlr_pos_impl &lt;- function(truth, estimate, estimator, event_level) {\n  xtab &lt;- table(estimate, truth)\n  # Rather than implement the actual method here, we rely on\n  # an *_estimator_impl() function that can handle binary\n  # and multiclass cases\n  lr_pos_estimator_impl(xtab, estimator, event_level)\n}\n\n\n# This function switches between binary and multiclass implementations\nlr_pos_estimator_impl &lt;- function(data, estimator, event_level) {\n  if(estimator == \"binary\") {\n    lr_pos_binary(data, event_level)\n  } else {\n    # Encapsulates the macro, macro weighted, and micro cases\n    # TODO\n  }\n}\n\n\n\nLR Neg\n\nlr_neg_impl &lt;- function(truth, estimate, estimator, event_level) {\n  xtab &lt;- table(estimate, truth)\n  # Rather than implement the actual method here, we rely on\n  # an *_estimator_impl() function that can handle binary\n  # and multiclass cases\n  lr_neg_estimator_impl(xtab, estimator, event_level)\n}\n\n\n# This function switches between binary and multiclass implementations\nlr_neg_estimator_impl &lt;- function(data, estimator, event_level) {\n  if(estimator == \"binary\") {\n    lr_neg_binary(data, event_level)\n  } else {\n    # Encapsulates the macro, macro weighted, and micro cases\n    # TODO\n  }\n}\n\n\n\n\nBinary Implementation\n\nLR Pos\n\nlr_pos_binary &lt;- function(data, event_level) {\n  col &lt;- event_col(data, event_level)\n  col2 &lt;- setdiff(colnames(data), col)\n  \n  tp &lt;- data[col, col]\n  tn &lt;- data[col2, col2]\n  fp &lt;- data[col, col2]\n  fn &lt;- data[col2, col]\n  # list(tp = tp, tn = tn, fp = fp, fn = fn)\n  (tp / (tp + fn)) / (fp / (fp + tn))\n  \n}\n\nlr_pos_binary(pathology_xtab, event_level = \"first\")\n#&gt; [1] 2.40625\n\n\n\nLR Neg\n\nlr_neg_binary &lt;- function(data, event_level) {\n  col &lt;- event_col(data, event_level)\n  col2 &lt;- setdiff(colnames(data), col)\n  \n  tp &lt;- data[col, col]\n  tn &lt;- data[col2, col2]\n  fp &lt;- data[col, col2]\n  fn &lt;- data[col2, col]\n  # list(tp = tp, tn = tn, fp = fp, fn = fn)\n  (fn / (tp + fn)) / (tn / (fp + tn))\n  \n}\n\nlr_neg_binary(pathology_xtab, event_level = \"first\")\n#&gt; [1] 0.1666667\n\n\n# Checking\npathology_xtab\n#&gt;         \n#&gt;          abnorm norm\n#&gt;   abnorm    231   32\n#&gt;   norm       27   54\ncolnames(pathology_xtab)\n#&gt; [1] \"abnorm\" \"norm\"\n\n# TP\npathology_xtab[\"abnorm\", \"abnorm\"]\n#&gt; [1] 231\n# TN\npathology_xtab[\"norm\", \"norm\"]\n#&gt; [1] 54\n# FP\npathology_xtab[\"abnorm\", \"norm\"]\n#&gt; [1] 32\n# FN\npathology_xtab[\"norm\", \"abnorm\"]\n#&gt; [1] 27\n\n\n\n\nMulticlass Implementation\n[TODO]\n\n\nVec implement\n\nLR Pos\n\nlr_pos_vec &lt;- function(truth,\n                       estimate,\n                       estimator = NULL,\n                       na_rm = TRUE,\n                       case_weights = NULL,\n                       event_level = \"first\",\n                       ...) {\n  # calls finalize_estimator_internal() internally\n  estimator &lt;- finalize_estimator(truth, estimator, metric_class = \"lr_pos\")\n\n  check_class_metric(truth, estimate, case_weights, estimator)\n\n  if (na_rm) {\n    result &lt;- yardstick_remove_missing(truth, estimate, case_weights)\n\n    truth &lt;- result$truth\n    estimate &lt;- result$estimate\n    case_weights &lt;- result$case_weights\n  } else if (yardstick_any_missing(truth, estimate, case_weights)) {\n    return(NA_real_)\n  }\n\n  lr_pos_impl(truth, estimate, estimator, event_level)\n}\n\nlr_pos_vec(pathology$pathology, pathology$scan)\n#&gt; [1] 2.40625\n\n\n\nLR Neg\n\nlr_neg_vec &lt;- function(truth,\n                       estimate,\n                       estimator = NULL,\n                       na_rm = TRUE,\n                       case_weights = NULL,\n                       event_level = \"first\",\n                       ...) {\n  # calls finalize_estimator_internal() internally\n  estimator &lt;- finalize_estimator(truth, estimator, metric_class = \"lr_neg\")\n\n  check_class_metric(truth, estimate, case_weights, estimator)\n\n  if (na_rm) {\n    result &lt;- yardstick_remove_missing(truth, estimate, case_weights)\n\n    truth &lt;- result$truth\n    estimate &lt;- result$estimate\n    case_weights &lt;- result$case_weights\n  } else if (yardstick_any_missing(truth, estimate, case_weights)) {\n    return(NA_real_)\n  }\n\n  lr_neg_impl(truth, estimate, estimator, event_level)\n}\n\nlr_neg_vec(pathology$pathology, pathology$scan)\n#&gt; [1] 0.1666667\n\n\n\n\nDF implement\n\n# LR Pos\nlr_pos &lt;- function(data, ...) {\n  UseMethod(\"lr_pos\")\n}\n\nlr_pos &lt;- new_class_metric(lr_pos, direction = \"maximize\")\n\n# LR Neg\nlr_neg &lt;- function(data, ...) {\n  UseMethod(\"lr_neg\")\n}\n\nlr_neg &lt;- new_class_metric(lr_neg, direction = \"minimize\")\n\n\nlr_pos.data.frame &lt;- function(data,\n                              truth,\n                              estimate,\n                              estimator = NULL,\n                              na_rm = TRUE,\n                              case_weights = NULL,\n                              event_level = \"first\",\n                              ...) {\n  class_metric_summarizer(\n    name = \"lr_pos\",\n    fn = lr_pos_vec,\n    data = data,\n    truth = !!rlang::enquo(truth),\n    estimate = !!rlang::enquo(estimate),\n    estimator = estimator,\n    na_rm = na_rm,\n    case_weights = !!rlang::enquo(case_weights),\n    event_level = event_level\n  )\n}\n\n\nlr_neg.data.frame &lt;- function(data,\n                              truth,\n                              estimate,\n                              estimator = NULL,\n                              na_rm = TRUE,\n                              case_weights = NULL,\n                              event_level = \"first\",\n                              ...) {\n  class_metric_summarizer(\n    name = \"lr_neg\",\n    fn = lr_neg_vec,\n    data = data,\n    truth = !!rlang::enquo(truth),\n    estimate = !!rlang::enquo(estimate),\n    estimator = estimator,\n    na_rm = na_rm,\n    case_weights = !!rlang::enquo(case_weights),\n    event_level = event_level\n  )\n}\n\n\n\nUsing lr_pos()\n\nlr_pos(pathology, truth = pathology, estimate = scan)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 lr_pos  binary          2.41\n\n\nlr_pos_vec(truth = pathology$pathology, estimate = pathology$scan)\n#&gt; [1] 2.40625\n\n\n\nUsing lr_neg()\n\nlr_neg(pathology, truth = pathology, estimate = scan)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 lr_neg  binary         0.167\n\n\nlr_neg_vec(truth = pathology$pathology, estimate = pathology$scan)\n#&gt; [1] 0.1666667\n\n\n\nUsing with metric_set()\n\nclass_metrics_2 &lt;- metric_set(accuracy, sens, spec, lr_pos, lr_neg)\nclass_metrics_2\n#&gt; # A tibble: 5 × 3\n#&gt;   metric   class        direction\n#&gt;   &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;    \n#&gt; 1 accuracy class_metric maximize \n#&gt; 2 sens     class_metric maximize \n#&gt; 3 spec     class_metric maximize \n#&gt; 4 lr_pos   class_metric maximize \n#&gt; 5 lr_neg   class_metric minimize\n\n\nconf_mat(pathology, truth = pathology, estimate = scan)\n#&gt;           Truth\n#&gt; Prediction abnorm norm\n#&gt;     abnorm    231   32\n#&gt;     norm       27   54\n\n\nclass_metrics_2(pathology, truth = pathology, estimate = scan)\n#&gt; # A tibble: 5 × 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.828\n#&gt; 2 sens     binary         0.895\n#&gt; 3 spec     binary         0.628\n#&gt; 4 lr_pos   binary         2.41 \n#&gt; 5 lr_neg   binary         0.167\n\nCheck LR+\n\n0.8953488   / (1 - 0.6279070) # LR+ = Sens / (1-Spec)\n#&gt; [1] 2.40625\n\nCheck LR-\n\n(1 - 0.8953488) / 0.6279070  # LR- = (1-Sens) / Spec\n#&gt; [1] 0.1666667"
  },
  {
    "objectID": "stats/perf-mat.html#pre-processing",
    "href": "stats/perf-mat.html#pre-processing",
    "title": "Multi-class Performance Matrix",
    "section": "Pre-processing",
    "text": "Pre-processing\n\nset.seed(1)\niris_split &lt;- initial_split(iris, prop = 0.7, strata = Species)\niris_tr &lt;- training(iris_split)\niris_tst &lt;- testing(iris_split)\n\nRecipes\n\niris_rec &lt;- recipe(Species ~ ., data = iris) \n\nModel Spec\n\nmultinom_sim &lt;- multinom_reg(engine = \"nnet\")\n\nWorkflow\n\niris_wf &lt;- workflow(iris_rec, multinom_sim)"
  },
  {
    "objectID": "stats/perf-mat.html#fit-predict",
    "href": "stats/perf-mat.html#fit-predict",
    "title": "Multi-class Performance Matrix",
    "section": "Fit & Predict",
    "text": "Fit & Predict\nFit Model\n\niris_fit &lt;- fit(iris_wf, data = iris_tr)\niris_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: multinom_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nCall:\nnnet::multinom(formula = ..y ~ ., data = data, trace = FALSE)\n\nCoefficients:\n           (Intercept) Sepal.Length Sepal.Width Petal.Length Petal.Width\nversicolor    53.35203     3.845927   -31.93694     10.02870   -3.275975\nvirginica    -57.85131   -23.541980   -41.71563     61.61759   31.272313\n\nResidual Deviance: 0.1554973 \nAIC: 20.1555 \n\n\nPredict\n\niris_res &lt;- broom::augment(iris_fit, new_data = iris_tst)\nhead(iris_res)\n\n# A tibble: 6 × 9\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species .pred_class\n         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;   &lt;fct&gt;      \n1          4.9         3            1.4         0.2 setosa  setosa     \n2          5           3.6          1.4         0.2 setosa  setosa     \n3          5.4         3.7          1.5         0.2 setosa  setosa     \n4          4.8         3            1.4         0.1 setosa  setosa     \n5          5.7         4.4          1.5         0.4 setosa  setosa     \n6          5.4         3.9          1.3         0.4 setosa  setosa     \n# ℹ 3 more variables: .pred_setosa &lt;dbl&gt;, .pred_versicolor &lt;dbl&gt;,\n#   .pred_virginica &lt;dbl&gt;"
  },
  {
    "objectID": "stats/perf-mat.html#multi-class-performace-matrix",
    "href": "stats/perf-mat.html#multi-class-performace-matrix",
    "title": "Multi-class Performance Matrix",
    "section": "Multi-class Performace Matrix",
    "text": "Multi-class Performace Matrix\n\nConfusion Matrix\n\nconf_mat(iris_res, truth = Species, estimate = .pred_class)\n\n            Truth\nPrediction   setosa versicolor virginica\n  setosa         14          0         0\n  versicolor      1         13         0\n  virginica       0          2        15\n\n\nCount number of observed class\n\nclass_totals &lt;- iris_res |&gt; \n  count(Species, name = \"totals\") %&gt;% \n  mutate(class_wts = totals / sum(totals))\n\nclass_totals\n\n# A tibble: 3 × 3\n  Species    totals class_wts\n  &lt;fct&gt;       &lt;int&gt;     &lt;dbl&gt;\n1 setosa         15     0.333\n2 versicolor     15     0.333\n3 virginica      15     0.333\n\n\n\ncell_counts &lt;- \n  iris_res %&gt;% \n  group_by(Species, .pred_class) %&gt;% \n  count() %&gt;% \n  ungroup()\n\ncell_counts\n\n# A tibble: 5 × 3\n  Species    .pred_class     n\n  &lt;fct&gt;      &lt;fct&gt;       &lt;int&gt;\n1 setosa     setosa         14\n2 setosa     versicolor      1\n3 versicolor versicolor     13\n4 versicolor virginica       2\n5 virginica  virginica      15\n\n\n\n# Compute the four sensitivities using 1-vs-all\none_versus_all &lt;- \n  cell_counts %&gt;% \n  filter(Species == .pred_class) %&gt;% \n  full_join(class_totals, by = \"Species\") %&gt;% \n  mutate(sens = n / totals)\n\none_versus_all\n\n# A tibble: 3 × 6\n  Species    .pred_class     n totals class_wts  sens\n  &lt;fct&gt;      &lt;fct&gt;       &lt;int&gt;  &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 setosa     setosa         14     15     0.333 0.933\n2 versicolor versicolor     13     15     0.333 0.867\n3 virginica  virginica      15     15     0.333 1    \n\n\n\n# Three different estimates:\none_versus_all %&gt;% \n  summarize(\n    macro = mean(sens), \n    macro_wts = weighted.mean(sens, class_wts),\n    micro = sum(n) / sum(totals)\n  )\n\n# A tibble: 1 × 3\n  macro macro_wts micro\n  &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 0.933     0.933 0.933\n\n\n\nMacro-averaging: computes a set of one-versus-all metrics using the standard two-class statistics. These are averaged.\nMacro-weighted averaging: does the same but the average is weighted by the number of samples in each class.\nMicro-averaging: computes the contribution for each class, aggregates them, then computes a single metric from the aggregates.\n\n\n\n\nMulticlass Doodles"
  },
  {
    "objectID": "ml/bestglm.html#regsubset",
    "href": "ml/bestglm.html#regsubset",
    "title": "Best GLM",
    "section": "Regsubset",
    "text": "Regsubset\n\ntrain &lt;- (zprostate[zprostate[, 10], ])[, -10]\nX &lt;- train[, 1:8]\ny &lt;- train[, 9]\n\n\nout &lt;- summary(regsubsets(x = X, y = y, nvmax = ncol(X)))\nSubsets &lt;- out$which\nRSS &lt;- out$rss\n\n\nzprostate_regsub_tbl &lt;-\n  cbind(as.data.frame(Subsets), RSS = RSS) |&gt; \n  relocate(RSS) |&gt; \n  mutate(across(where(is.logical), ~ifelse(.x, \"*\", NA)))\n\nzprostate_regsub_tbl\n\n       RSS (Intercept) lcavol lweight  age lbph  svi  lcp gleason pgg45\n1 44.52858           *      *    &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n2 37.09185           *      *       * &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n3 34.90775           *      *       * &lt;NA&gt; &lt;NA&gt;    * &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n4 32.81499           *      *       * &lt;NA&gt;    *    * &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n5 32.06945           *      *       * &lt;NA&gt;    *    * &lt;NA&gt;    &lt;NA&gt;     *\n6 30.53978           *      *       * &lt;NA&gt;    *    *    *    &lt;NA&gt;     *\n7 29.43730           *      *       *    *    *    *    *    &lt;NA&gt;     *\n8 29.42638           *      *       *    *    *    *    *       *     *"
  },
  {
    "objectID": "ml/bestglm.html#best-glm",
    "href": "ml/bestglm.html#best-glm",
    "title": "Best GLM",
    "section": "Best GLM",
    "text": "Best GLM\n\nLinear Reg\n\n# `y` must be the last column\nXy &lt;- cbind(as.data.frame(X), lpsa = y)\n\nzprostate_bestglm &lt;- bestglm(Xy)\nnames(zprostate_bestglm)\n\n[1] \"BestModel\"   \"BestModels\"  \"Bestq\"       \"qTable\"      \"Subsets\"    \n[6] \"Title\"       \"ModelReport\"\n\n\nBestModel: lm or glm object giving the best model\n\nzprostate_bestglm$BestModel\n\n\nCall:\nlm(formula = y ~ ., data = data.frame(Xy[, c(bestset[-1], FALSE), \n    drop = FALSE], y = y))\n\nCoefficients:\n(Intercept)       lcavol      lweight  \n     2.4774       0.7397       0.3163  \n\n\nBestModels: a 𝑇 × 𝑝 logical matrix showing which variables are included in the top 𝑇 models\n\nzprostate_bestglm$BestModels\n\n  lcavol lweight   age  lbph   svi   lcp gleason pgg45 Criterion\n1   TRUE    TRUE FALSE FALSE FALSE FALSE   FALSE FALSE -31.20741\n2   TRUE    TRUE FALSE FALSE  TRUE FALSE   FALSE FALSE -31.06884\n3   TRUE    TRUE FALSE  TRUE  TRUE FALSE   FALSE FALSE -31.00630\n4   TRUE    TRUE FALSE FALSE FALSE FALSE   FALSE  TRUE -30.06625\n5   TRUE   FALSE FALSE  TRUE  TRUE FALSE   FALSE FALSE -29.06138\n\n\nSubsets: a (𝑝 + 1) × 𝑝 logical matrix showing which variables are included for subset sizes 𝑘 = 0, . . . , 𝑝 have the smallest deviance\n\nzprostate_bestglm$Subsets\n\n   (Intercept) lcavol lweight   age  lbph   svi   lcp gleason pgg45\n0         TRUE  FALSE   FALSE FALSE FALSE FALSE FALSE   FALSE FALSE\n1         TRUE   TRUE   FALSE FALSE FALSE FALSE FALSE   FALSE FALSE\n2*        TRUE   TRUE    TRUE FALSE FALSE FALSE FALSE   FALSE FALSE\n3         TRUE   TRUE    TRUE FALSE FALSE  TRUE FALSE   FALSE FALSE\n4         TRUE   TRUE    TRUE FALSE  TRUE  TRUE FALSE   FALSE FALSE\n5         TRUE   TRUE    TRUE FALSE  TRUE  TRUE FALSE   FALSE  TRUE\n6         TRUE   TRUE    TRUE FALSE  TRUE  TRUE  TRUE   FALSE  TRUE\n7         TRUE   TRUE    TRUE  TRUE  TRUE  TRUE  TRUE   FALSE  TRUE\n8         TRUE   TRUE    TRUE  TRUE  TRUE  TRUE  TRUE    TRUE  TRUE\n   logLikelihood       BIC\n0      -12.14653  24.29306\n1       13.68680 -23.16892\n2*      19.80840 -31.20741\n3       21.84146 -31.06884\n4       23.91254 -31.00630\n5       24.68243 -28.34139\n6       26.31970 -27.41124\n7       27.55141 -25.66996\n8       27.56383 -21.49012\n\n\nqTable: a table showing all possible model choices for different intervals of 𝑞.\n\nzprostate_bestglm$qTable\n\n          LogL           q1           q2 k\n[1,] -12.14653 0.000000e+00 4.940404e-11 0\n[2,]  13.68680 4.940404e-11 1.764939e-02 1\n[3,]  19.80840 1.764939e-02 5.125667e-01 2\n[4,]  23.91254 5.125667e-01 7.087642e-01 4\n[5,]  27.55141 7.087642e-01 8.899197e-01 7\n[6,]  27.56383 8.899197e-01 1.000000e+00 8\n\n\n\n\nLog Reg\n\nlibrary(ISLR2)\ndata(\"Default\")\n\n\nDefault_Xy &lt;- Default |&gt; relocate(default, .after = last_col())\n\n\nDefault_b.glm &lt;- bestglm(Default_Xy, family = binomial(), \n                         IC = \"BIC\")\n\nMorgan-Tatar search since family is non-gaussian.\n\n\nBestModel: lm or glm object giving the best model\n\nDefault_b.glm$BestModel\n\n\nCall:  glm(formula = y ~ ., family = family, data = Xi, weights = weights)\n\nCoefficients:\n(Intercept)   studentYes      balance  \n -10.749496    -0.714878     0.005738  \n\nDegrees of Freedom: 9999 Total (i.e. Null);  9997 Residual\nNull Deviance:      2921 \nResidual Deviance: 1572     AIC: 1578\n\n\nBestModels: a 𝑇 × 𝑝 logical matrix showing which variables are included in the top 𝑇 models\n\nDefault_b.glm$BestModels\n\n  student balance income Criterion\n1    TRUE    TRUE  FALSE  1590.102\n2   FALSE    TRUE   TRUE  1597.387\n3    TRUE    TRUE   TRUE  1599.176\n4   FALSE    TRUE  FALSE  1605.662\n5    TRUE   FALSE  FALSE  2917.893\n\n\n\n\nLog Reg (Penguin)\n\nlibrary(palmerpenguins)\ndata(penguins)\n# Only work with this\npenguins_Xy &lt;- penguins |&gt; \n  relocate(sex, .after = last_col()) |&gt; \n  select(ends_with(\"_mm\"), body_mass_g, sex) |&gt; \n  mutate(across(where(is.integer), as.double)) |&gt; \n  filter(!if_any(everything(), ~is.na(.x))) |&gt; \n  as.data.frame() # Must\n\nhead(penguins_Xy)\n\n  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g    sex\n1           39.1          18.7               181        3750   male\n2           39.5          17.4               186        3800 female\n3           40.3          18.0               195        3250 female\n4           36.7          19.3               193        3450 female\n5           39.3          20.6               190        3650   male\n6           38.9          17.8               181        3625 female\n\n\n\npenguins_bestglm &lt;- bestglm::bestglm(penguins_Xy, \n                                     family = binomial())\n\nMorgan-Tatar search since family is non-gaussian.\n\n\n\npenguins_bestglm$BestModels\n\n  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g Criterion\n1          FALSE          TRUE             FALSE        TRUE  175.9541\n2           TRUE          TRUE             FALSE        TRUE  177.3122\n3          FALSE          TRUE              TRUE        TRUE  181.7523\n4           TRUE          TRUE              TRUE        TRUE  182.2360\n5           TRUE          TRUE              TRUE       FALSE  268.3962"
  },
  {
    "objectID": "ml/tidymod-overview.html#explore-data",
    "href": "ml/tidymod-overview.html#explore-data",
    "title": "Tidymodels Overview",
    "section": "Explore Data",
    "text": "Explore Data\nOutcome: species\n\nglimpse(pen)\n\nRows: 344\nColumns: 5\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n\n\n\npen |&gt;\n  filter(!is.na(sex)) |&gt;\n  ggplot(aes(x     = flipper_length_mm,\n             y     = bill_depth_mm,\n             color = species,\n             size  = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~sex)\n\n\n\n\nFigure 11.1: ?(caption)\n\n\n\n\n\npen |&gt; \n  count(species)\n\n# A tibble: 3 × 2\n  species       n\n  &lt;fct&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\nComplete record rate:\n\nvapply(pen, function(x) mean(!is.na(x)), numeric(1))\n\n          species     bill_depth_mm flipper_length_mm       body_mass_g \n        1.0000000         0.9941860         0.9941860         0.9941860 \n              sex \n        0.9680233"
  },
  {
    "objectID": "ml/tidymod-overview.html#data-budget",
    "href": "ml/tidymod-overview.html#data-budget",
    "title": "Tidymodels Overview",
    "section": "Data Budget",
    "text": "Data Budget\n\nSplit Data\n\nset.seed(123)\npen_split &lt;- initial_split(pen, prop = 0.8, strata = species)\npen_split\n\n&lt;Training/Testing/Total&gt;\n&lt;274/70/344&gt;\n\npen_train &lt;- training(pen_split)\npen_test &lt;- testing(pen_split)\n\n\n\nResample\n10-folded CV, repeated 2 times from the training data\n\nset.seed(123)\npen_folds &lt;- vfold_cv(pen_train, v = 10)\n\nhead(pen_folds)\n\n# A tibble: 6 × 2\n  splits           id    \n  &lt;list&gt;           &lt;chr&gt; \n1 &lt;split [246/28]&gt; Fold01\n2 &lt;split [246/28]&gt; Fold02\n3 &lt;split [246/28]&gt; Fold03\n4 &lt;split [246/28]&gt; Fold04\n5 &lt;split [247/27]&gt; Fold05\n6 &lt;split [247/27]&gt; Fold06"
  },
  {
    "objectID": "ml/tidymod-overview.html#recipes",
    "href": "ml/tidymod-overview.html#recipes",
    "title": "Tidymodels Overview",
    "section": "Recipes",
    "text": "Recipes\n\npen_rec_base &lt;- recipe(species ~ ., data = pen_train) \npen_rec_base\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4"
  },
  {
    "objectID": "ml/tidymod-overview.html#model-spec",
    "href": "ml/tidymod-overview.html#model-spec",
    "title": "Tidymodels Overview",
    "section": "Model Spec",
    "text": "Model Spec\n\nmspec_cls &lt;- list(\n  multi_nnet = multinom_reg(engine = \"nnet\"),\n  multi_glmnet_lasso = multinom_reg(engine = \"glmnet\", \n                                    penalty = 0.1, mixture = 1)\n)\n\nmap(mspec_cls, translate)\n\n$multi_nnet\nMultinomial Regression Model Specification (classification)\n\nComputational engine: nnet \n\nModel fit template:\nnnet::multinom(formula = missing_arg(), data = missing_arg(), \n    trace = FALSE)\n\n$multi_glmnet_lasso\nMultinomial Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = 0.1\n  mixture = 1\n\nComputational engine: glmnet \n\nModel fit template:\nglmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    alpha = 1, family = \"multinomial\")"
  },
  {
    "objectID": "ml/tidymod-overview.html#workflow",
    "href": "ml/tidymod-overview.html#workflow",
    "title": "Tidymodels Overview",
    "section": "Workflow",
    "text": "Workflow\n\nSingle\n\nworkflow(preprocessor = pen_rec_base, spec = mspec_cls$multi_nnet)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: multinom_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nMultinomial Regression Model Specification (classification)\n\nComputational engine: nnet \n\n\n\n\nCombinations\n\npen_wfset &lt;- workflow_set(\n  preproc = list(base = pen_rec_base),\n  models = mspec_cls\n)\n\npen_wfset\n\n# A workflow set/tibble: 2 × 4\n  wflow_id                info             option    result    \n  &lt;chr&gt;                   &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 base_multi_nnet         &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 base_multi_glmnet_lasso &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n\n\n\npen_wf_base &lt;- extract_workflow(pen_wfset, id = \"base_multi_nnet\")\npen_wf_base\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: multinom_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nMultinomial Regression Model Specification (classification)\n\nComputational engine: nnet"
  },
  {
    "objectID": "ml/tidymod-overview.html#other-spec",
    "href": "ml/tidymod-overview.html#other-spec",
    "title": "Tidymodels Overview",
    "section": "Other Spec",
    "text": "Other Spec\n\nPerformance Matric Spec\nA function factory\n\nmet_set_class &lt;- metric_set(\n  accuracy, sensitivity, specificity,\n  mcc # Matthews correlation coefficient\n)\n\nmet_set_mix &lt;- metric_set(roc_auc, accuracy, sensitivity, specificity)\n\n\n\nResamples Control\n\nkeep_pred &lt;- control_resamples(save_pred = TRUE, save_workflow = TRUE)"
  },
  {
    "objectID": "ml/tidymod-overview.html#fit",
    "href": "ml/tidymod-overview.html#fit",
    "title": "Tidymodels Overview",
    "section": "Fit",
    "text": "Fit\n\nUsing: Test Data\n\npen_fit_base &lt;- fit(pen_wf_base, data = pen_test)\npen_fit_base\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: multinom_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nCall:\nnnet::multinom(formula = ..y ~ ., data = data, trace = FALSE)\n\nCoefficients:\n          (Intercept) bill_depth_mm flipper_length_mm  body_mass_g   sexmale\nChinstrap -21.7033319     -0.555911         0.1906063 -0.001596491 0.7168018\nGentoo     -0.4884471     -6.002835         0.2447407  0.011754103 3.8349607\n\nResidual Deviance: 46.77568 \nAIC: 66.77568 \n\n\n\n\nUsing: Resamples\n\n# Unix and macOS only\nlibrary(doMC)\n\nLoading required package: foreach\n\n\n\nAttaching package: 'foreach'\n\n\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n\n\nLoading required package: iterators\n\n\nLoading required package: parallel\n\nregisterDoMC(cores = 4)\n\n\npen_fit_fold_base &lt;- fit_resamples(pen_wf_base, \n                                   resamples = pen_folds,\n                                   metrics = met_set_mix,\n                                   control = keep_pred)\nhead(pen_fit_fold_base)\n\n# A tibble: 6 × 5\n  splits           id     .metrics         .notes           .predictions     \n  &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;           \n1 &lt;split [246/28]&gt; Fold01 &lt;tibble [4 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [28 × 7]&gt;\n2 &lt;split [246/28]&gt; Fold02 &lt;tibble [4 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [28 × 7]&gt;\n3 &lt;split [246/28]&gt; Fold03 &lt;tibble [4 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [28 × 7]&gt;\n4 &lt;split [246/28]&gt; Fold04 &lt;tibble [4 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [28 × 7]&gt;\n5 &lt;split [247/27]&gt; Fold05 &lt;tibble [4 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [27 × 7]&gt;\n6 &lt;split [247/27]&gt; Fold06 &lt;tibble [4 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [27 × 7]&gt;"
  },
  {
    "objectID": "ml/tidymod-overview.html#evaluate-predict",
    "href": "ml/tidymod-overview.html#evaluate-predict",
    "title": "Tidymodels Overview",
    "section": "Evaluate & Predict",
    "text": "Evaluate & Predict\n\nUsing: Test set\n\nPredict\n\npen_res_base &lt;- broom::augment(pen_fit_base, new_data = pen_test)\nglimpse(pen_res_base)\n\nRows: 70\nColumns: 9\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, 17.1, 20.7, 18.4, 17.9, 17.8, 21.1…\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, 186, 197, 184, 187, 188, 196, 179, 19…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, 3300, 4500, 3325, 3200, 3300, 4150…\n$ sex               &lt;fct&gt; male, female, female, NA, male, female, female, fema…\n$ .pred_class       &lt;fct&gt; Adelie, Adelie, Chinstrap, NA, Adelie, Adelie, Adeli…\n$ .pred_Adelie      &lt;dbl&gt; 0.9463005, 0.8797500, 0.4329285, NA, 0.8936564, 0.89…\n$ .pred_Chinstrap   &lt;dbl&gt; 0.05369946, 0.12024966, 0.56707149, NA, 0.10634361, …\n$ .pred_Gentoo      &lt;dbl&gt; 1.147426e-09, 3.454101e-07, 6.534172e-11, NA, 2.2390…\n\n\n\n\nMetric\n\nmet_set_class(pen_res_base, \n              truth = species, \n              estimate = .pred_class,\n              estimator = \"macro\" # Macro AVG\n              )\n\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    multiclass     0.809\n2 sensitivity macro          0.727\n3 specificity macro          0.896\n4 mcc         multiclass     0.704\n\n\n\n\nROC\n\nroc_curve(pen_res_base, truth = species, \n          .pred_Adelie, .pred_Chinstrap, .pred_Gentoo) |&gt; \n  autoplot()\n\n\n\n\nGentoo curve is on the top-left see Figure 11.1 for reason.\n\nroc_auc(pen_res_base, truth = species, \n        .pred_Adelie, .pred_Chinstrap, .pred_Gentoo)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc hand_till      0.933\n\n\n\n\n\nUsing Resamples\n\nMetric\n\ncollect_metrics(pen_fit_fold_base)\n\n# A tibble: 4 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    multiclass 0.823    10  0.0227 Preprocessor1_Model1\n2 roc_auc     hand_till  0.918    10  0.0166 Preprocessor1_Model1\n3 sensitivity macro      0.753    10  0.0231 Preprocessor1_Model1\n4 specificity macro      0.904    10  0.0110 Preprocessor1_Model1\n\n\nThese are the resampling estimates averaged over the individual replicates. To get the metrics for each resample, use the option summarize = FALSE.\n\n\nPredictions\nAssessment set predictions:\n\npen_assess_base &lt;- collect_predictions(pen_fit_fold_base, summarize = FALSE)\nhead(pen_assess_base)\n\n# A tibble: 6 × 8\n  id     .pred_Adelie .pred_Chinstrap .pred_Gentoo  .row .pred_class species\n  &lt;chr&gt;         &lt;dbl&gt;           &lt;dbl&gt;        &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;  \n1 Fold01        0.792          0.208      4.02e-25    10 Adelie      Adelie \n2 Fold01        0.691          0.309      3.25e-18    17 Adelie      Adelie \n3 Fold01        0.982          0.0177     7.24e-12    19 Adelie      Adelie \n4 Fold01        0.979          0.0209     2.11e-22    22 Adelie      Adelie \n5 Fold01        0.846          0.154      4.46e-12    46 Adelie      Adelie \n6 Fold01        0.880          0.120      5.82e-14    51 Adelie      Adelie \n# ℹ 1 more variable: .config &lt;chr&gt;\n\n\n.row column is an integer that matches the row of the original training set so that these results can be properly arranged and joined with the original data.\nAveraged Predictions:\n\ncollect_predictions(pen_fit_fold_base, summarize = T) |&gt; head()\n\n# A tibble: 6 × 7\n   .row species .config    .pred_Adelie .pred_Chinstrap .pred_Gentoo .pred_class\n  &lt;int&gt; &lt;fct&gt;   &lt;chr&gt;             &lt;dbl&gt;           &lt;dbl&gt;        &lt;dbl&gt; &lt;fct&gt;      \n1     1 Adelie  Preproces…      NaN            NaN        NaN        Adelie     \n2     2 Adelie  Preproces…        0.514          0.486      1.41e-17 Adelie     \n3     3 Adelie  Preproces…        0.874          0.126      8.77e-21 Adelie     \n4     4 Adelie  Preproces…        0.949          0.0509     1.53e-14 Adelie     \n5     5 Adelie  Preproces…        0.791          0.209      4.07e-13 Adelie     \n6     6 Adelie  Preproces…      NaN            NaN        NaN        Adelie     \n\n\n\n\nROC\n\npen_assess_base |&gt; \n  group_by(id) |&gt; \n  roc_curve(truth = species, .pred_Adelie, .pred_Chinstrap, .pred_Gentoo) |&gt; \n  autoplot()"
  },
  {
    "objectID": "ml/ml-ops.html#eda",
    "href": "ml/ml-ops.html#eda",
    "title": "ML Ops with Penguin",
    "section": "EDA",
    "text": "EDA\n\npenguins |&gt;\n  filter(!is.na(sex)) |&gt;\n  ggplot(aes(x     = flipper_length_mm,\n             y     = bill_length_mm,\n             color = sex,\n             size  = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~species)"
  },
  {
    "objectID": "ml/ml-ops.html#split",
    "href": "ml/ml-ops.html#split",
    "title": "ML Ops with Penguin",
    "section": "Split",
    "text": "Split\n\n# remove rows with missing sex, exclude year and island\npenguins_df &lt;-\n  palmerpenguins::penguins |&gt;\n  drop_na(sex) |&gt;\n  select(-year, -island)\n\n# set the seed for reproducibility\nset.seed(1234)\n\n# Split the data into train and test sets stratified by sex\npenguin_split &lt;- initial_split(penguins_df, strata = sex)\npenguin_train &lt;- training(penguin_split)\npenguin_test  &lt;- testing(penguin_split)\n\n# create folds for cross validation\npenguin_folds &lt;- vfold_cv(penguin_train)"
  },
  {
    "objectID": "ml/ml-ops.html#recipes",
    "href": "ml/ml-ops.html#recipes",
    "title": "ML Ops with Penguin",
    "section": "Recipes",
    "text": "Recipes\n\npenguin_rec &lt;-\n  recipe(sex ~ ., data = penguin_train) |&gt;     \n  step_YeoJohnson(all_numeric_predictors()) |&gt; \n  step_dummy(species) |&gt;                       \n  step_normalize(all_numeric_predictors())"
  },
  {
    "objectID": "ml/ml-ops.html#model-spec",
    "href": "ml/ml-ops.html#model-spec",
    "title": "ML Ops with Penguin",
    "section": "Model Spec",
    "text": "Model Spec\n\n# Logistic Regression\nglm_spec &lt;-\n  logistic_reg(penalty = 1) |&gt;\n  set_engine(\"glm\")\n\n# Random Forest\ntree_spec &lt;-\n  rand_forest(min_n = tune()) |&gt;\n  set_engine(\"ranger\") |&gt;\n  set_mode(\"classification\")\n\n# Neural Network with `{torch}` (Not Done)"
  },
  {
    "objectID": "ml/ml-ops.html#fit-models-tune-hyperparameters",
    "href": "ml/ml-ops.html#fit-models-tune-hyperparameters",
    "title": "ML Ops with Penguin",
    "section": "Fit Models & Tune Hyperparameters",
    "text": "Fit Models & Tune Hyperparameters\nUse Bayes optimizaiton for hyperparameter tuning\n\nbayes_control &lt;- control_bayes(no_improve = 10L,\n                               time_limit = 20,\n                               save_pred  = TRUE,\n                               verbose    = TRUE)\n\n\n# Unix and macOS only\nlibrary(doMC)\n\nLoading required package: foreach\n\n\n\nAttaching package: 'foreach'\n\n\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n\n\nLoading required package: iterators\n\n\nLoading required package: parallel\n\nregisterDoMC(cores = 8)\n\n\nworkflow_set &lt;-\n  workflow_set(\n    preproc = list(penguin_rec),\n    models  = list(glm   = glm_spec,\n                   tree  = tree_spec)\n  ) |&gt;\n  workflow_map(\"tune_bayes\",\n               iter      = 50L,\n               resamples = penguin_folds,\n               control   = bayes_control\n  )\n\n\n\n\n❯  Generating a set of 5 initial parameter results\n\n\n✓ Initialization complete\n\n\n\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 34 candidates\n\n\ni Predicted candidates\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 33 candidates\n\n\ni Predicted candidates\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 32 candidates\n\n\ni Predicted candidates\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 31 candidates\n\n\ni Predicted candidates\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 30 candidates\n\n\ni Predicted candidates\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 29 candidates\n\n\ni Predicted candidates\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 28 candidates\n\n\ni Predicted candidates\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 27 candidates\n\n\ni Predicted candidates\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 26 candidates\n\n\ni Predicted candidates\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 25 candidates\n\n\ni Predicted candidates\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 24 candidates\n\n\ni Predicted candidates\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\ni Gaussian process model\n\n\n✓ Gaussian process model\n\n\ni Generating 23 candidates\n\n\ni Predicted candidates\n\n\ni Estimating performance\n\n\n✓ Estimating performance\n\n\n! No improvement for 10 iterations; returning current results.\n\nclass(workflow_set)\n\n[1] \"workflow_set\" \"tbl_df\"       \"tbl\"          \"data.frame\"  \n\nworkflow_set\n\n# A workflow set/tibble: 2 × 4\n  wflow_id    info             option    result   \n  &lt;chr&gt;       &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n1 recipe_glm  &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;rsmp[+]&gt;\n2 recipe_tree &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;tune[+]&gt;"
  },
  {
    "objectID": "ml/ml-ops.html#compare-model-results",
    "href": "ml/ml-ops.html#compare-model-results",
    "title": "ML Ops with Penguin",
    "section": "Compare Model Results",
    "text": "Compare Model Results\n\nTabular view\n\n# create table of best models defined using roc_auc metric\nrank_results(workflow_set,\n             rank_metric = \"roc_auc\",\n             select_best = TRUE)\n\n# A tibble: 4 × 9\n  wflow_id    .config       .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_glm  Preprocessor… accura… 0.900  0.0199    10 recipe       logi…     1\n2 recipe_glm  Preprocessor… roc_auc 0.969  0.0123    10 recipe       logi…     1\n3 recipe_tree Iter2         accura… 0.912  0.0251    10 recipe       rand…     2\n4 recipe_tree Iter2         roc_auc 0.967  0.0132    10 recipe       rand…     2\n\n\n\n\nPlotting performance\n\nautoplot(workflow_set)"
  },
  {
    "objectID": "ml/ml-ops.html#finalize",
    "href": "ml/ml-ops.html#finalize",
    "title": "ML Ops with Penguin",
    "section": "Finalize",
    "text": "Finalize\n\nSelect best model\n\nbest_model_id &lt;- \"recipe_glm\"\n\nbest_fit &lt;-\n  workflow_set |&gt;\n  extract_workflow_set_result(best_model_id) |&gt;\n  select_best(metric = \"accuracy\")\n\nbest_fit\n\n# A tibble: 1 × 1\n  .config             \n  &lt;chr&gt;               \n1 Preprocessor1_Model1\n\n\n\n\nFinal Fit\n\n# create workflow for best model\nfinal_workflow &lt;-\n  workflow_set |&gt;\n  extract_workflow(best_model_id) |&gt;\n  finalize_workflow(best_fit)\n\n# fit final model with all data\nfinal_fit &lt;-\n  final_workflow |&gt;\n  last_fit(penguin_split)\n\n\n\nFinal Metric\n\n# show model performance\ncollect_metrics(final_fit)\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.905 Preprocessor1_Model1\n2 roc_auc  binary         0.971 Preprocessor1_Model1\n\n\n\ncollect_predictions(final_fit) |&gt;\n  roc_curve(sex, .pred_female) |&gt; \n  autoplot()"
  },
  {
    "objectID": "ml/compare-mod.html#data",
    "href": "ml/compare-mod.html#data",
    "title": "Comparing Models",
    "section": "Data",
    "text": "Data\n\ndata(ames)\names &lt;- ames %&gt;% mutate(Sale_Price = log10(Sale_Price))\n\nset.seed(502)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\names_folds &lt;- vfold_cv(ames_train, v = 10)"
  },
  {
    "objectID": "ml/compare-mod.html#recipes",
    "href": "ml/compare-mod.html#recipes",
    "title": "Comparing Models",
    "section": "Recipes",
    "text": "Recipes\n\nbasic_rec &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_other(Neighborhood, threshold = 0.01) %&gt;% \n  step_dummy(all_nominal_predictors())\n\ninteraction_rec &lt;- \n  basic_rec %&gt;% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) \n\nspline_rec &lt;- \n  interaction_rec %&gt;% \n  step_ns(Latitude, Longitude, deg_free = 50)\n\n# List of Recipes \npreproc &lt;- \n  list(basic = basic_rec, \n       interact = interaction_rec, \n       splines = spline_rec\n  )"
  },
  {
    "objectID": "ml/compare-mod.html#models",
    "href": "ml/compare-mod.html#models",
    "title": "Comparing Models",
    "section": "Models",
    "text": "Models\n\nmspecs &lt;- list(\n  lm = linear_reg()\n)\n\nclass(mspecs$lm)\n\n[1] \"linear_reg\" \"model_spec\""
  },
  {
    "objectID": "ml/compare-mod.html#workflows",
    "href": "ml/compare-mod.html#workflows",
    "title": "Comparing Models",
    "section": "Workflows",
    "text": "Workflows\n\nlm_models &lt;- workflow_set(preproc, mspecs, cross = FALSE)\nclass(lm_models)\n\n[1] \"workflow_set\" \"tbl_df\"       \"tbl\"          \"data.frame\"  \n\nlm_models\n\n# A workflow set/tibble: 3 × 4\n  wflow_id    info             option    result    \n  &lt;chr&gt;       &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 basic_lm    &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 interact_lm &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n3 splines_lm  &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;"
  },
  {
    "objectID": "ml/compare-mod.html#fit-workflows",
    "href": "ml/compare-mod.html#fit-workflows",
    "title": "Comparing Models",
    "section": "Fit Workflows",
    "text": "Fit Workflows\n\nlm_wfs &lt;- \n  lm_models %&gt;% \n  workflow_map(\"fit_resamples\", \n               # Options to `workflow_map()`: \n               seed = 1101, verbose = TRUE,\n               # Options to `fit_resamples()`: \n               resamples = ames_folds, \n               control = control_resamples(save_pred = T))\n\ni 1 of 3 resampling: basic_lm\n\n\n✔ 1 of 3 resampling: basic_lm (1.8s)\n\n\ni 2 of 3 resampling: interact_lm\n\n\n✔ 2 of 3 resampling: interact_lm (1.8s)\n\n\ni 3 of 3 resampling: splines_lm\n\n\n✔ 3 of 3 resampling: splines_lm (3.2s)\n\nclass(lm_wfs)\n\n[1] \"workflow_set\" \"tbl_df\"       \"tbl\"          \"data.frame\"  \n\nlm_wfs\n\n# A workflow set/tibble: 3 × 4\n  wflow_id    info             option    result   \n  &lt;chr&gt;       &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n1 basic_lm    &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n2 interact_lm &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n3 splines_lm  &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;"
  },
  {
    "objectID": "ml/compare-mod.html#metric",
    "href": "ml/compare-mod.html#metric",
    "title": "Comparing Models",
    "section": "Metric",
    "text": "Metric\nMetric for each workflows\n\nlm_wfs_metrics &lt;- collect_metrics(lm_wfs)\nlm_wfs_metrics\n\n# A tibble: 6 × 9\n  wflow_id    .config      preproc model .metric .estimator   mean     n std_err\n  &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 basic_lm    Preprocesso… recipe  line… rmse    standard   0.0804    10 0.00313\n2 basic_lm    Preprocesso… recipe  line… rsq     standard   0.793     10 0.0122 \n3 interact_lm Preprocesso… recipe  line… rmse    standard   0.0800    10 0.00301\n4 interact_lm Preprocesso… recipe  line… rsq     standard   0.795     10 0.0114 \n5 splines_lm  Preprocesso… recipe  line… rmse    standard   0.0786    10 0.00288\n6 splines_lm  Preprocesso… recipe  line… rsq     standard   0.802     10 0.0120"
  },
  {
    "objectID": "ml/tune-mod.html#split",
    "href": "ml/tune-mod.html#split",
    "title": "Tune Models",
    "section": "Split",
    "text": "Split\n\names &lt;- make_ames()\n\nset.seed(4595)\ndata_split &lt;- initial_split(ames, strata = \"Sale_Price\")\n\names_train &lt;- training(data_split)\n\nset.seed(2453)\nrs_splits &lt;- vfold_cv(ames_train, strata = \"Sale_Price\")"
  },
  {
    "objectID": "ml/tune-mod.html#recipes",
    "href": "ml/tune-mod.html#recipes",
    "title": "Tune Models",
    "section": "Recipes",
    "text": "Recipes\n\names_rec &lt;-\n  recipe(Sale_Price ~ ., data = ames_train) %&gt;%\n  step_log(Sale_Price, base = 10) %&gt;%\n  step_YeoJohnson(Lot_Area, Gr_Liv_Area) %&gt;%\n  step_other(Neighborhood, threshold = .1)  %&gt;%\n  step_dummy(all_nominal()) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_ns(Longitude, deg_free = tune(\"lon\")) %&gt;%\n  step_ns(Latitude, deg_free = tune(\"lat\"))"
  },
  {
    "objectID": "ml/tune-mod.html#models",
    "href": "ml/tune-mod.html#models",
    "title": "Tune Models",
    "section": "Models",
    "text": "Models\n\nknn_model &lt;-\n  nearest_neighbor(\n    mode = \"regression\",\n    neighbors = tune(\"K\"),\n    weight_func = tune(),\n    dist_power = tune()\n  ) %&gt;%\n  set_engine(\"kknn\")"
  },
  {
    "objectID": "ml/tune-mod.html#workflow-parameters",
    "href": "ml/tune-mod.html#workflow-parameters",
    "title": "Tune Models",
    "section": "Workflow & Parameters",
    "text": "Workflow & Parameters\n\names_wflow &lt;-\n  workflow() %&gt;%\n  add_recipe(ames_rec) %&gt;%\n  add_model(knn_model)\n\nclass(ames_wflow)\n\n[1] \"workflow\"\n\n\n\names_set &lt;-\n  extract_parameter_set_dials(ames_wflow) %&gt;%\n  update(K = neighbors(c(1, 50)))\n\nclass(ames_set)\n\n[1] \"parameters\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\names_set\n\nCollection of 5 parameters for tuning\n\n  identifier        type    object\n           K   neighbors nparam[+]\n weight_func weight_func dparam[+]\n  dist_power  dist_power nparam[+]\n         lon    deg_free nparam[+]\n         lat    deg_free nparam[+]"
  },
  {
    "objectID": "ml/tune-mod.html#grid",
    "href": "ml/tune-mod.html#grid",
    "title": "Tune Models",
    "section": "Grid",
    "text": "Grid\n\nParameter Grids\n\nset.seed(7014)\n\n### Space-filling parameter grids\names_grid &lt;-\n  ames_set %&gt;%\n  grid_max_entropy(size = 10)\n\names_grid\n\n# A tibble: 10 × 5\n       K weight_func  dist_power   lon   lat\n   &lt;int&gt; &lt;chr&gt;             &lt;dbl&gt; &lt;int&gt; &lt;int&gt;\n 1    35 optimal           1.32      8     1\n 2    35 rank              1.29      3    13\n 3    21 cos               0.626     1     4\n 4     4 biweight          0.311     8     4\n 5    32 triangular        0.165     9    15\n 6     3 rank              1.86     10    15\n 7    40 triangular        0.167    11     7\n 8    12 epanechnikov      1.53      4     7\n 9     5 rank              0.411     2     7\n10    33 triweight         0.511    10     3\n\n\n\n\nGrid Search !\n### Perform Grid Search (Not Run)\n\names_grid_search &lt;-\n  tune_grid(\n    ames_wflow,\n    resamples = rs_splits,\n    grid = ames_grid\n  )\n\ndata(\"example_ames_knn\")\nclass(ames_grid_search)\n\n[1] \"tune_results\" \"tbl_df\"       \"tbl\"          \"data.frame\"  \n\names_grid_search\n\n# Tuning results\n# 10-fold cross-validation using stratification \n# A tibble: 10 × 4\n   splits           id     .metrics          .notes          \n   &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          \n 1 &lt;split [1978/0]&gt; Fold01 &lt;tibble [20 × 9]&gt; &lt;tibble [0 × 1]&gt;\n 2 &lt;split [1979/0]&gt; Fold02 &lt;tibble [20 × 9]&gt; &lt;tibble [0 × 1]&gt;\n 3 &lt;split [1979/0]&gt; Fold03 &lt;tibble [20 × 9]&gt; &lt;tibble [0 × 1]&gt;\n 4 &lt;split [1979/0]&gt; Fold04 &lt;tibble [20 × 9]&gt; &lt;tibble [0 × 1]&gt;\n 5 &lt;split [1979/0]&gt; Fold05 &lt;tibble [20 × 9]&gt; &lt;tibble [0 × 1]&gt;\n 6 &lt;split [1979/0]&gt; Fold06 &lt;tibble [20 × 9]&gt; &lt;tibble [0 × 1]&gt;\n 7 &lt;split [1979/0]&gt; Fold07 &lt;tibble [20 × 9]&gt; &lt;tibble [0 × 1]&gt;\n 8 &lt;split [1979/0]&gt; Fold08 &lt;tibble [20 × 9]&gt; &lt;tibble [0 × 1]&gt;\n 9 &lt;split [1979/0]&gt; Fold09 &lt;tibble [20 × 9]&gt; &lt;tibble [0 × 1]&gt;\n10 &lt;split [1981/0]&gt; Fold10 &lt;tibble [20 × 9]&gt; &lt;tibble [0 × 1]&gt;"
  },
  {
    "objectID": "ml/tune-mod.html#finalized",
    "href": "ml/tune-mod.html#finalized",
    "title": "Tune Models",
    "section": "Finalized",
    "text": "Finalized\n\nSelect Best Tune Result\n\nlowest_rmse &lt;- select_best(ames_grid_search, metric = \"rmse\")\nclass(lowest_rmse)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nlowest_rmse\n\n# A tibble: 1 × 6\n      K weight_func dist_power   lon   lat .config              \n  &lt;int&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1    33 triweight        0.511    10     3 Preprocessor10_Model1\n\n\n\n\nLast Fit\n\names_res_last &lt;- workflow(ames_rec, knn_model) |&gt; \n  finalize_workflow(lowest_rmse) |&gt; \n  last_fit(split = data_split, metrics = metric_set(rmse))\n\n\nclass(ames_res_last)\n\n[1] \"last_fit\"         \"resample_results\" \"tune_results\"     \"tbl_df\"          \n[5] \"tbl\"              \"data.frame\"      \n\names_res_last\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits             id               .metrics .notes   .predictions .workflow \n  &lt;list&gt;             &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [2197/733]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;"
  },
  {
    "objectID": "misc/parallel.html#parallel",
    "href": "misc/parallel.html#parallel",
    "title": "Parallel Processing",
    "section": "{parallel}",
    "text": "{parallel}\n\n# The number of physical cores in the hardware:\nparallel::detectCores(logical = FALSE)\n\n[1] 4\n\n# The number of possible independent processes that can \n# be simultaneously used:  \nparallel::detectCores(logical = TRUE)\n\n[1] 8"
  },
  {
    "objectID": "misc/parallel.html#domc",
    "href": "misc/parallel.html#domc",
    "title": "Parallel Processing",
    "section": "{doMC}",
    "text": "{doMC}\n\n# Unix and macOS only\nlibrary(doMC)\n\nLoading required package: foreach\n\n\nLoading required package: iterators\n\n\nLoading required package: parallel\n\nregisterDoMC(cores = 8)\n\n# Now run fit_resamples()...\n\nregisterDoSEQ() # Reset"
  },
  {
    "objectID": "misc/parallel.html#doparallel",
    "href": "misc/parallel.html#doparallel",
    "title": "Parallel Processing",
    "section": "{doParallel}",
    "text": "{doParallel}\n\n# All operating systems\nlibrary(doParallel)\n# Create a cluster object and then register: \ncl &lt;- makePSOCKcluster(8)\nregisterDoParallel(cl)\n\n## Run\n\n# Reset\nstopCluster(cl)"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  }
]